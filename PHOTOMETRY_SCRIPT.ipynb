{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f2ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from copy import copy\n",
    "from timeit import default_timer\n",
    "from typing import Dict, List, Optional, Callable, Tuple, Union, TypeVar, Any\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from astropy.utils.exceptions import AstropyDeprecationWarning, AstropyUserWarning\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from astropy.nddata import NDData\n",
    "from astropy.modeling import fitting\n",
    "from astropy.wcs.utils import proj_plane_pixel_area\n",
    "import multiprocessing\n",
    "\n",
    "FILE = \"FILENAME\"\n",
    "LOCAL_ARCHIEVE = \"/Users/kimphan/Desktop/flows_test/photometry/2018rw\"\n",
    "FOLDER_OUTPUT = \"/Users/kimphan/Desktop/flows_test/photometry_test_output\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc1654",
   "metadata": {},
   "source": [
    "# ··###Import magnitudes.py ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ca2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download utilities, then filters.py, then magnitudes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb124abc",
   "metadata": {},
   "source": [
    "# utilities.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f814ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    ".. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "\"\"\"\n",
    "import hashlib\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "from typing import Optional\n",
    "\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "\n",
    "def get_filehash(fname):\n",
    "    \"\"\"Calculate SHA1-hash of file.\"\"\"\n",
    "    buf = 65536\n",
    "    s = hashlib.sha1()\n",
    "    with open(fname, 'rb') as fid:\n",
    "        while True:\n",
    "            data = fid.read(buf)\n",
    "            if not data:\n",
    "                break\n",
    "            s.update(data)\n",
    "\n",
    "    sha1sum = s.hexdigest().lower()\n",
    "    if len(sha1sum) != 40:\n",
    "        raise Exception(\"Invalid file hash\")\n",
    "    return sha1sum\n",
    "\n",
    "\n",
    "def has_file_handler(logger):\n",
    "    \"\"\"Check if logger has one file handler.\"\"\"\n",
    "    return sum([type(l) is logging.FileHandler for l in logger.handlers]) > 0\n",
    "\n",
    "\n",
    "def has_stream_handler(logger):\n",
    "    \"\"\"Check if logger has one stream handler.\"\"\"\n",
    "    return sum([type(l) is logging.StreamHandler for l in logger.handlers]) > 0\n",
    "\n",
    "\n",
    "def remove_file_handlers(logger):\n",
    "    \"\"\"Remove file handler from logger.\"\"\"\n",
    "    for handler in logger.handlers:\n",
    "        if type(handler) is logging.FileHandler:\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "\n",
    "def create_logger(worker_name=None, log_level: Optional[int] = None, log_file: str = None):\n",
    "    formatter = logging.Formatter('[%(asctime)s| %(levelname)s| %(processName)s | %(module)s] %(message)s')\n",
    "    if worker_name is not None:\n",
    "        process = multiprocessing.current_process()\n",
    "        process.name = worker_name\n",
    "\n",
    "    logger = multiprocessing.get_logger()\n",
    "    if log_level is not None:\n",
    "        logger.setLevel(log_level)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    # remove duplicated messages in the output\n",
    "    if not has_stream_handler(logger):\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    # check for log file and remove duplicated messages in the output\n",
    "    if log_file is not None and not has_file_handler(logger):\n",
    "        file_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s -%(module)s - %(message)s', \"%Y-%m-%d %H:%M:%S\")\n",
    "        file_handler = logging.FileHandler(log_file, mode='w')\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def create_warning_logger(log_file: str):\n",
    "    \"\"\"Create a logger for warnings.\"\"\"\n",
    "    logging.captureWarnings(True)\n",
    "    logger_warn = logging.getLogger('py.warnings')\n",
    "\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s -%(module)s - %(message)s', \"%Y-%m-%d %H:%M:%S\")\n",
    "    file_handler = logging.FileHandler(log_file, mode='w')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "\n",
    "    if not has_file_handler(logger_warn):\n",
    "        logger_warn.addHandler(file_handler)\n",
    "\n",
    "    return logger_warn\n",
    "\n",
    "\n",
    "def parse_log_level(args: Namespace):\n",
    "    logging_level = logging.INFO\n",
    "    if args.quiet:\n",
    "        logging_level = logging.WARNING\n",
    "    elif args.debug:\n",
    "        logging_level = logging.DEBUG\n",
    "    return logging_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841bc10",
   "metadata": {},
   "source": [
    "# filters.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb21834",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filters.py ###\n",
    "from typing import Optional\n",
    "#from .utilities import create_logger\n",
    "logger = create_logger()\n",
    "FILTERS = {\n",
    "    'up': 'u_mag',\n",
    "    'gp': 'g_mag',\n",
    "    'rp': 'r_mag',\n",
    "    'ip': 'i_mag',\n",
    "    'zp': 'z_mag',\n",
    "    'B': 'B_mag',\n",
    "    'V': 'V_mag',\n",
    "    'J': 'J_mag',\n",
    "    'H': 'H_mag',\n",
    "    'K': 'K_mag',\n",
    "}\n",
    "\n",
    "FALLBACK_FILTER = 'gp'\n",
    "\n",
    "\n",
    "def get_reference_filter(photfilter: str) -> str:\n",
    "    \"\"\"\n",
    "    Translate photometric filter into table column.\n",
    "\n",
    "    Parameters:\n",
    "        photfilter (str): photometric filter corresponding to key of FILTERS\n",
    "    \"\"\"\n",
    "\n",
    "    _ref_filter = FILTERS.get(photfilter, None)\n",
    "    if _ref_filter is None:\n",
    "        logger.warning(f\"Could not find filter {photfilter} in catalogs. \"\n",
    "                       f\"Using default {FALLBACK_FILTER} filter.\")\n",
    "        _ref_filter = FILTERS[FALLBACK_FILTER]\n",
    "    return _ref_filter\n",
    "\n",
    "def clean_value(value: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean value.\n",
    "    \"\"\"\n",
    "    return value.replace(' ', '').replace('-', '').replace('.', '').replace('_', '').lower()\n",
    "\n",
    "COMMON_FILTERS = {\n",
    "    'B': 'B', 'V': 'V', 'R': 'R', 'g': 'gp', 'r': 'rp', \n",
    "    'i': 'ip', 'u': 'up', 'z': 'zp',\n",
    "    'Ks': 'K', 'Hs': 'H', 'Js': 'J',\n",
    "    'Bessel-B': 'B', 'Bessel-V': 'V', 'Bessell-V': 'V', 'SDSS-U': 'up',\n",
    "    'SDSS-G': 'gp', 'SDSS-R': 'rp', 'SDSS-I': 'ip', 'SDSS-Z': 'zp',\n",
    "    'PS1-u': 'up', 'PS1-g': 'gp', 'PS1-r': 'rp', 'PS1-i': 'ip', 'PS1-z': 'zp',\n",
    "    'PS2-u': 'up', 'PS2-g': 'gp', 'PS2-r': 'rp', 'PS2-i': 'ip', 'PS2-z': 'zp',\n",
    "    'PS-u': 'up', 'PS-g': 'gp', 'PS-r': 'rp', 'PS-i': 'ip', 'PS-z': 'zp',\n",
    "    'Yc': 'Y', 'Jc': 'J', 'Hc': 'H', 'Kc': 'K',\n",
    "    'Yo': 'Y', 'Jo': 'J', 'Ho': 'H', 'Ko': 'K',\n",
    "    \"J_Open\": \"J\", \"H_Open\": \"H\", \"K_Open\": \"K\",\n",
    "    \"B_Open\": \"B\", \"V_Open\": \"V\", \"R_Open\": \"r\", \"I_Open\": \"i\", \"Y_Open\": \"Y\",\n",
    "    \"g_Open\": \"gp\", \"r_Open\": \"rp\", \"i_Open\": \"ip\", \"z_Open\": \"zp\", 'u_Open': 'up',\n",
    "}\n",
    "\n",
    "\n",
    "COMMON_FILTERS_LOWER = {clean_value(key): value for key, value in COMMON_FILTERS.items()}\n",
    "              \n",
    "                  \n",
    "def match_header_to_filter(header_dict: dict[str,str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract flows filter from header.\n",
    "    \"\"\"\n",
    "    bad_keys = [\"\", \"NONE\", \"Clear\"]\n",
    "    filt = header_dict.get(\"FILTER\")\n",
    "    if filt is not None and filt not in bad_keys:\n",
    "        filt = match_filter_to_flows(filt)\n",
    "        if filt is not None:\n",
    "            return filt    \n",
    "    \n",
    "    for key, value in header_dict.items():\n",
    "        if \"FILT\" in key.upper():\n",
    "            if value not in bad_keys:\n",
    "                filt = match_filter_to_flows(value)\n",
    "                if filt is not None:\n",
    "                    return filt\n",
    "        \n",
    "   \n",
    "    raise ValueError(\"Could not determine filter from header. Add FILTER keyword with a flows filter to header.\")\n",
    "                  \n",
    "\n",
    "def match_filter_to_flows(header_filter: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Match filter header value to flows filter.\n",
    "    \"\"\"\n",
    "    if header_filter in FILTERS:\n",
    "        return header_filter\n",
    "\n",
    "    values = max(header_filter.lower().split(' '), header_filter.lower().split('.'), key=len)\n",
    "    \n",
    "    \n",
    "    filters_keys_lower = [str(key).lower() for key in FILTERS.keys()]\n",
    "    for value in values:\n",
    "        if value in filters_keys_lower:\n",
    "            if FILTERS.get(value) is not None:\n",
    "                return value\n",
    "            return value.upper()\n",
    "         \n",
    "    \n",
    "    filters_keys_lower = [clean_value(str(key)) for key in COMMON_FILTERS.keys()]\n",
    "    for value in values:\n",
    "        clean = clean_value(value)\n",
    "        if clean in filters_keys_lower:\n",
    "            return COMMON_FILTERS_LOWER.get(clean) \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bac70",
   "metadata": {},
   "source": [
    "# import target.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bbf3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "from numpy.typing import NDArray\n",
    "from tendrils import api\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Target:\n",
    "    ra: float\n",
    "    dec: float\n",
    "    name: Optional[str] = None\n",
    "    id: Optional[int] = None  # Target id from Flows database\n",
    "    photfilter: Optional[str] = None  # Defined if target is associated with an image.\n",
    "    coords: Optional[SkyCoord] = None\n",
    "    pixel_column: Optional[int] = None\n",
    "    pixel_row: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.coords is None:\n",
    "            self.coords = SkyCoord(ra=self.ra, dec=self.dec, unit='deg', frame='icrs')\n",
    "\n",
    "    def calc_pixels(self, wcs: WCS) -> None:\n",
    "        pixels = np.array(wcs.all_world2pix(self.ra, self.dec, 1)).T\n",
    "        self._add_pixel_coordinates(pixel_pos=pixels)\n",
    "\n",
    "    def _add_pixel_coordinates(self, pixel_column: Optional[int] = None, pixel_row: Optional[int] = None,\n",
    "                               pixel_pos: Optional[NDArray] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add pixel coordinates to target.\n",
    "        \"\"\"\n",
    "        if pixel_column is None or pixel_row is None:\n",
    "            if pixel_pos is None:\n",
    "                raise ValueError('Either pixel_column, pixel_row or pixel_pos must be provided.')\n",
    "            pixel_column, pixel_row = pixel_pos\n",
    "\n",
    "        self.pixel_column = pixel_column\n",
    "        self.pixel_row = pixel_row\n",
    "\n",
    "    def output_dict(self, starid: Optional[int] = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        Return target as output dictionary. starid = -1 means difference image.\n",
    "        \"\"\"\n",
    "        return {'starid': starid, 'ra': self.ra, 'decl': self.dec, 'pixel_column': self.pixel_column,\n",
    "                'pixel_row': self.pixel_row, 'used_for_epsf': False}\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict) -> 'Target':\n",
    "        \"\"\"\n",
    "        Create target from dictionary.\n",
    "        \"\"\"\n",
    "        return cls(ra=d['ra'], dec=d['decl'], name=d['target_name'], id=d['targetid'], photfilter=d['photfilter'],)\n",
    "\n",
    "    @classmethod\n",
    "    def from_fid(cls, fid: int, datafile: Optional[Dict] = None) -> 'Target':\n",
    "        \"\"\"\n",
    "        Create target from fileid.\n",
    "        \"\"\"\n",
    "\n",
    "        datafile = datafile or api.get_datafile(fid)\n",
    "        if datafile is None:\n",
    "            raise ValueError(f'No datafile found for fid={fid}')\n",
    "        d = api.get_target(datafile['target_name']) | datafile\n",
    "        return cls.from_dict(d)\n",
    "\n",
    "    @classmethod\n",
    "    def from_tid(cls, target_id: int) -> 'Target':\n",
    "        \"\"\"\n",
    "        Create target from target id.\n",
    "        \"\"\"\n",
    "        target_pars = api.get_target(target_id)\n",
    "        return cls(\n",
    "            ra=target_pars['ra'], dec=target_pars['decl'],\n",
    "            name=target_pars['target_name'], id=target_pars['targetid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6fd75",
   "metadata": {},
   "source": [
    "# import zeropoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef92b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Provides functions for computing the zeropoint and its error.\n",
    "Uses bootstrapping with sigma clipping as outlier rejection,\n",
    "where the sigma is determined by the Chauvenet criteria. Also\n",
    "allows for arbitrary outlier and fitting functions.\n",
    "\n",
    ".. codeauthor:: Emir Karamehmetoglu <emir.k@phys.au.dk>\n",
    "\"\"\"\n",
    "from typing import List, Optional, Dict, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "from astropy.stats import bootstrap\n",
    "from astropy.modeling import fitting\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.special import erfcinv\n",
    "\n",
    "\n",
    "# Calculate sigma for sigma clipping using Chauvenet\n",
    "def sigma_from_Chauvenet(Nsamples):\n",
    "    '''Calculate sigma according to the Cheuvenet criterion'''\n",
    "    return erfcinv(1. / (2 * Nsamples)) * (2.) ** (1 / 2)\n",
    "\n",
    "\n",
    "def bootstrap_outlier(x: ArrayLike, y: ArrayLike, yerr: ArrayLike, n: int = 500, model='None',\n",
    "                      fitter: Union[Callable, str] = None, outlier='None', outlier_kwargs: Optional[Dict] = None,\n",
    "                      summary: Union[Callable, str] = 'median', error: Union[Callable, str] = 'bootstrap',\n",
    "                      parnames: Optional[List] = None, return_vals: bool = True):\n",
    "    \"\"\"x = catalog mag, y = instrumental mag, yerr = instrumental error\n",
    "    summary = function for summary statistic, np.nanmedian by default.\n",
    "    model = Linear1D\n",
    "    fitter = LinearLSQFitter\n",
    "    outlier = 'sigma_clip'\n",
    "    outlier_kwargs, default sigma = 3\n",
    "    return_vals = False will return dictionary\n",
    "    Performs bootstrap with replacement and returns model.\n",
    "    \"\"\"\n",
    "    summary = np.nanmedian if summary == 'median' else summary\n",
    "    error = np.nanstd if error == 'bootstrap' else error\n",
    "    parnames = ['intercept'] if parnames is None else parnames\n",
    "    outlier_kwargs = {'sigma': 3} if outlier_kwargs is None else outlier_kwargs\n",
    "\n",
    "    # Create index for bootstrapping\n",
    "    ind = np.arange(len(x))\n",
    "\n",
    "    # Bootstrap indexes with replacement using astropy\n",
    "    bootstraps = bootstrap(ind, bootnum=n)\n",
    "    bootstraps.sort()  # sort increasing.\n",
    "    bootinds = bootstraps.astype(int)\n",
    "\n",
    "    # Prepare fitter\n",
    "    fitter = fitting.LinearLSQFitter if fitter is None else fitter\n",
    "    fitter_instance = fitting.FittingWithOutlierRemoval(fitter(), outlier, **outlier_kwargs)\n",
    "    # Fit each bootstrap with model and fitter using outlier rejection at each step.\n",
    "    # Then obtain summary statistic for each parameter in parnames\n",
    "    pars = {}\n",
    "    out = {}\n",
    "    for parname in parnames:\n",
    "        pars[parname] = np.ones(len(bootinds), dtype=np.float64)\n",
    "    for i, bs in enumerate(bootinds):\n",
    "        # w = np.ones(len(x[bs]), dtype=np.float64) if yerr=='None' else (1.0/yerr[bs])**2\n",
    "        w = (1.0 / yerr[bs]) ** 2\n",
    "        best_fit, sigma_clipped = fitter_instance(model, x[bs], y[bs], weights=w)\n",
    "        # obtain parameters of interest\n",
    "        for parname in parnames:\n",
    "            pars[parname][i] = best_fit.parameters[np.array(best_fit.param_names) == parname][0]\n",
    "    if return_vals:\n",
    "        return [summary(pars[par]) for par in pars]\n",
    "\n",
    "    for parname in parnames:\n",
    "        out[parname] = summary(pars[parname])\n",
    "        out[parname + '_error'] = error(pars[parname])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d7ba0",
   "metadata": {},
   "source": [
    "# magnitudes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243901d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.table import Table\n",
    "from bottleneck import nansum\n",
    "\n",
    "#from filters import get_reference_filter\n",
    "#from target import Target\n",
    "#from utilities import create_logger\n",
    "#from zeropoint import sigma_from_Chauvenet, bootstrap_outlier\n",
    "\n",
    "logger = create_logger()\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def instrumental_mag(tab: Table, target: Target, make_fig: bool = False) -> Tuple[Table, Optional[plt.Figure],\n",
    "                                                                                  Optional[plt.Axes]]:\n",
    "    target_rows = tab['starid'] <= 0\n",
    "\n",
    "    # Check that we got valid flux photometry:\n",
    "    if np.any(~np.isfinite(tab[target_rows]['flux_psf'])) or np.any(~np.isfinite(tab[target_rows]['flux_psf_error'])):\n",
    "        raise RuntimeError(f\"Target:{target.name} flux is undefined.\")\n",
    "\n",
    "    # Convert PSF fluxes to magnitudes:\n",
    "    mag_inst = -2.5 * np.log10(tab['flux_psf'])\n",
    "    mag_inst_err = (2.5 / np.log(10)) * (tab['flux_psf_error'] / tab['flux_psf'])\n",
    "\n",
    "    # Corresponding magnitudes in catalog:\n",
    "    mag_catalog = tab[get_reference_filter(target.photfilter)]\n",
    "\n",
    "    # Mask out things that should not be used in calibration:\n",
    "    use_for_calibration = np.ones_like(mag_catalog, dtype='bool')\n",
    "    use_for_calibration[target_rows] = False  # Do not use target for calibration\n",
    "    use_for_calibration[~np.isfinite(mag_inst) | ~np.isfinite(mag_catalog)] = False\n",
    "\n",
    "\n",
    "    # Just creating some short-hands:\n",
    "    x = mag_catalog[use_for_calibration]\n",
    "    y = mag_inst[use_for_calibration]\n",
    "    yerr = mag_inst_err[use_for_calibration]\n",
    "    weights = 1.0 / yerr ** 2\n",
    "\n",
    "    if not any(use_for_calibration):\n",
    "        raise RuntimeError(\"No calibration stars\")\n",
    "\n",
    "    # Fit linear function with fixed slope, using sigma-clipping:\n",
    "    model = models.Linear1D(slope=1, fixed={'slope': True})\n",
    "    fitter = fitting.FittingWithOutlierRemoval(fitting.LinearLSQFitter(), sigma_clip, sigma=3.0)\n",
    "    best_fit, sigma_clipped = fitter(model, x, y, weights=weights)\n",
    "\n",
    "    # Extract zero-point and estimate its error using a single weighted fit:\n",
    "    # I don't know why there is not an error-estimate attached directly to the Parameter?\n",
    "    zp = -1 * best_fit.intercept.value  # Negative, because that is the way zeropoints are usually defined\n",
    "\n",
    "    weights[sigma_clipped] = 0  # Trick to make following expression simpler\n",
    "    n_weights = len(weights.nonzero()[0])\n",
    "    if n_weights > 1:\n",
    "        zp_error = np.sqrt(n_weights * nansum(weights * (y - best_fit(x)) ** 2) / nansum(weights) / (n_weights - 1))\n",
    "    else:\n",
    "        zp_error = np.NaN\n",
    "    logger.info('Leastsquare ZP = %.3f, ZP_error = %.3f', zp, zp_error)\n",
    "\n",
    "    # Determine sigma clipping sigma according to Chauvenet method\n",
    "    # But don't allow less than sigma = sigmamin, setting to 1.5 for now.\n",
    "    # Should maybe be 2?\n",
    "    sigmamin = 1.5\n",
    "    sig_chauv = sigma_from_Chauvenet(len(x))\n",
    "    sig_chauv = sig_chauv if sig_chauv >= sigmamin else sigmamin\n",
    "\n",
    "    # Extract zero point and error using bootstrap method\n",
    "    nboot = 1000\n",
    "    logger.info('Running bootstrap with sigma = %.2f and n = %d', sig_chauv, nboot)\n",
    "    pars = bootstrap_outlier(x, y, yerr, n=nboot, model=model, fitter=fitting.LinearLSQFitter, outlier=sigma_clip,\n",
    "                             outlier_kwargs={'sigma': sig_chauv}, summary='median', error='bootstrap',\n",
    "                             return_vals=False)\n",
    "\n",
    "    zp_bs = pars['intercept'] * -1.0\n",
    "    zp_error_bs = pars['intercept_error']\n",
    "\n",
    "    logger.info('Bootstrapped ZP = %.3f, ZP_error = %.3f', zp_bs, zp_error_bs)\n",
    "\n",
    "    # Check that difference is not large\n",
    "    zp_diff = 0.4\n",
    "    if np.abs(zp_bs - zp) >= zp_diff:\n",
    "        logger.warning(\"Bootstrap and weighted LSQ ZPs differ by %.2f, \"\n",
    "                       \"which is more than the allowed %.2f mag.\", np.abs(zp_bs - zp), zp_diff)\n",
    "\n",
    "    # Add calibrated magnitudes to the photometry table:\n",
    "    tab['mag'] = mag_inst + zp_bs\n",
    "    tab['mag_error'] = np.sqrt(mag_inst_err ** 2 + zp_error_bs ** 2)\n",
    "\n",
    "    # Check that we got valid magnitude photometry:\n",
    "    if not np.isfinite(tab[0]['mag']) or not np.isfinite(tab[0]['mag_error']):\n",
    "        raise RuntimeError(f\"Target:{target.name} magnitude is undefined.\")\n",
    "\n",
    "    # Update Meta-data:\n",
    "    tab.meta['zp'] = zp_bs\n",
    "    tab.meta['zp_error'] = zp_error_bs\n",
    "    tab.meta['zp_diff'] = np.abs(zp_bs - zp)\n",
    "    tab.meta['zp_error_weights'] = zp_error\n",
    "\n",
    "    # Plot:\n",
    "    if make_fig:\n",
    "        mag_fig, mag_ax = plt.subplots(1, 1)\n",
    "        mag_ax.errorbar(x, y, yerr=yerr, fmt='k.')\n",
    "        mag_ax.scatter(x[sigma_clipped], y[sigma_clipped], marker='x', c='r')\n",
    "        mag_ax.plot(x, best_fit(x), color='g', linewidth=3)\n",
    "        mag_ax.set_xlabel('Catalog magnitude')\n",
    "        mag_ax.set_ylabel('Instrumental magnitude')\n",
    "\n",
    "        return tab, mag_fig, mag_ax\n",
    "    return tab, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f1982",
   "metadata": {},
   "source": [
    "### import result_model###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b0cef",
   "metadata": {},
   "source": [
    "# import image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb68779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from typing import Union\n",
    "from astropy.time import Time\n",
    "from astropy.wcs import WCS, FITSFixedWarning\n",
    "from typing import Tuple,  Dict, Any, Optional, TypeGuard\n",
    "#from .utilities import create_logger\n",
    "logger = create_logger()\n",
    "\n",
    "@dataclass\n",
    "class InstrumentDefaults:\n",
    "    \"\"\"\n",
    "    Default radius and FWHM for an instrument in arcseconds.\n",
    "    \"\"\"\n",
    "    radius: float = 10\n",
    "    fwhm: float = 6.0   # Best initial guess\n",
    "    fwhm_min: float = 3.5\n",
    "    fwhm_max: float = 18.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FlowsImage:\n",
    "    image: np.ndarray\n",
    "    header: Dict\n",
    "    mask: Optional[np.ndarray] = None\n",
    "    peakmax: Optional[float] = None\n",
    "    exptime: Optional[float] = None\n",
    "    instrument_defaults: Optional[InstrumentDefaults] = None\n",
    "    site: Optional[Dict[str, Any]] = None\n",
    "    obstime: Optional[Time] = None\n",
    "    photfilter: Optional[str] = None\n",
    "    wcs: Optional[WCS] = None\n",
    "    fwhm: Optional[float] = None\n",
    "    fid: Optional[int] = None  # FileID of this image\n",
    "    template_fid: Optional[int] = None  # Template file ID if exists in same band.\n",
    "\n",
    "    clean: Optional[np.ma.MaskedArray] = None\n",
    "    subclean: Optional[np.ma.MaskedArray] = None\n",
    "    error: Optional[np.ma.MaskedArray] = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.shape = self.image.shape\n",
    "        self.wcs = self.create_wcs()\n",
    "        # Create mask\n",
    "        self.initialize_mask()\n",
    "\n",
    "    def initialize_mask(self) -> None:\n",
    "        self.update_mask(self.mask)\n",
    "\n",
    "    def check_finite(self) -> None:\n",
    "        if self.ensure_mask(self.mask):\n",
    "            self.mask |= ~np.isfinite(self.image)\n",
    "\n",
    "    def mask_non_linear(self) -> None:\n",
    "        if self.peakmax is None:\n",
    "            return\n",
    "        if self.ensure_mask(self.mask):\n",
    "            self.mask |= self.image >= self.peakmax\n",
    "\n",
    "    def ensure_mask(self, mask: Optional[np.ndarray]) -> TypeGuard[NDArray[np.bool_]]:\n",
    "        if mask is None:\n",
    "            self.mask = np.zeros_like(self.image, dtype='bool')\n",
    "        return True\n",
    "\n",
    "    def update_mask(self, mask) -> None:\n",
    "        self.mask = mask\n",
    "        self.check_finite()\n",
    "        self.mask_non_linear()\n",
    "\n",
    "    def create_wcs(self) -> WCS:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', category=FITSFixedWarning)\n",
    "            return WCS(header=self.header, relax=True)\n",
    "\n",
    "    def create_masked_image(self) -> None:\n",
    "        \"\"\"Warning: this is destructive and will overwrite image data setting masked values to NaN\"\"\"\n",
    "        self.image[self.mask] = np.NaN\n",
    "        self.clean = np.ma.masked_array(data=self.image, mask=self.mask, copy=False)\n",
    "\n",
    "    def set_edge_rows_to_value(self, y: Tuple[float] = None, value: Union[int, float, np.float64] = 0) -> None:\n",
    "        if y is None:\n",
    "            pass\n",
    "        for row in y:\n",
    "            self.image[row] = value\n",
    "\n",
    "    def set_edge_columns_to_value(self, x: Tuple[float] = None, value: Union[int, float, np.float64] = 0) -> None:\n",
    "        if x is None:\n",
    "            pass\n",
    "        for col in x:\n",
    "            self.image[:, col] = value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_edge_mask(img: np.ndarray, value: Union[int, float, np.float64] = 0):\n",
    "        \"\"\"\n",
    "        Create boolean mask of given value near edge of image.\n",
    "\n",
    "        Parameters:\n",
    "            img (ndarray): image with values for masking.\n",
    "            value (float): Value to detect near edge. Default=0.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Pixel mask with given values on the edge of image.\n",
    "\n",
    "        .. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "        \"\"\"\n",
    "\n",
    "        mask1 = (img == value)\n",
    "        mask = np.zeros_like(img, dtype='bool')\n",
    "\n",
    "        # Mask entire rows and columns which are only the value:\n",
    "        mask[np.all(mask1, axis=1), :] = True\n",
    "        mask[:, np.all(mask1, axis=0)] = True\n",
    "\n",
    "        # Detect \"uneven\" edges column-wise in image:\n",
    "        a = np.argmin(mask1, axis=0)\n",
    "        b = np.argmin(np.flipud(mask1), axis=0)\n",
    "        for col in range(img.shape[1]):\n",
    "            if mask1[0, col]:\n",
    "                mask[:a[col], col] = True\n",
    "            if mask1[-1, col]:\n",
    "                mask[-b[col]:, col] = True\n",
    "\n",
    "        # Detect \"uneven\" edges row-wise in image:\n",
    "        a = np.argmin(mask1, axis=1)\n",
    "        b = np.argmin(np.fliplr(mask1), axis=1)\n",
    "        for row in range(img.shape[0]):\n",
    "            if mask1[row, 0]:\n",
    "                mask[row, :a[row]] = True\n",
    "            if mask1[row, -1]:\n",
    "                mask[row, -b[row]:] = True\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def apply_edge_mask(self, y: Tuple[int] = None, x: Tuple[int] = None, apply_existing_mask_first: bool = False):\n",
    "        \"\"\"\n",
    "        Masks given rows and columns of image but will replace the current mask! Set apply_existing_mask_first to True\n",
    "        if the current mask should be kept.\n",
    "        :param y: Tuple[int] of rows to mask\n",
    "        :param x: Tuple[int] of columns to mask\n",
    "        :param apply_existing_mask_first: Whether to apply the existing mask to image first, before overwriting mask.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if y is None and x is None:\n",
    "            logger.debug(\"(y,x) was None when applying edge mask. Edge was not actually masked.\")\n",
    "\n",
    "        if apply_existing_mask_first:\n",
    "            self.create_masked_image()\n",
    "\n",
    "        if y is not None:\n",
    "            self.set_edge_rows_to_value(y=y)\n",
    "\n",
    "        if x is not None:\n",
    "            self.set_edge_columns_to_value(x=x)\n",
    "\n",
    "        self.mask = self.get_edge_mask(self.image)\n",
    "        self.create_masked_image()\n",
    "\n",
    "class ImageType(Enum):\n",
    "    raw = 'raw'\n",
    "    diff = 'diff'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e35912",
   "metadata": {},
   "source": [
    "# result_model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99720e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "#from .image import FlowsImage\n",
    "#from .utilities import create_logger\n",
    "logger = create_logger()\n",
    "class ResultsTable(Table):\n",
    "\n",
    "    def add_column_descriptions(self):\n",
    "        # Descriptions of columns:\n",
    "        self['used_for_epsf'].description = 'Was object used for building ePSF?'\n",
    "        self['mag'].description = 'Measured magnitude'\n",
    "        self['mag'].unit = u.mag\n",
    "        self['mag_error'].description = 'Error on measured magnitude'\n",
    "        self['mag_error'].unit = u.mag\n",
    "        self['flux_aperture'].description = 'Measured flux using aperture photometry'\n",
    "        self['flux_aperture'].unit = u.count / u.second\n",
    "        self['flux_aperture_error'].description = 'Error on measured flux using aperture photometry'\n",
    "        self['flux_aperture_error'].unit = u.count / u.second\n",
    "        self['flux_psf'].description = 'Measured flux using PSF photometry'\n",
    "        self['flux_psf'].unit = u.count / u.second\n",
    "        self['flux_psf_error'].description = 'Error on measured flux using PSF photometry'\n",
    "        self['flux_psf_error'].unit = u.count / u.second\n",
    "        self['pixel_column'].description = 'Location on image pixel columns'\n",
    "        self['pixel_column'].unit = u.pixel\n",
    "        self['pixel_row'].description = 'Location on image pixel rows'\n",
    "        self['pixel_row'].unit = u.pixel\n",
    "        self['pixel_column_psf_fit'].description = 'Measured location on image pixel columns from PSF photometry'\n",
    "        self['pixel_column_psf_fit'].unit = u.pixel\n",
    "        self['pixel_column_psf_fit_error'].description = 'Error on measured location on image pixel columns from PSF ' \\\n",
    "                                                         'photometry'\n",
    "        self['pixel_column_psf_fit_error'].unit = u.pixel\n",
    "        self['pixel_row_psf_fit'].description = 'Measured location on image pixel rows from PSF photometry'\n",
    "        self['pixel_row_psf_fit'].unit = u.pixel\n",
    "        self['pixel_row_psf_fit_error'].description = 'Error on measured location on image pixel rows from PSF ' \\\n",
    "                                                      'photometry'\n",
    "        self['pixel_row_psf_fit_error'].unit = u.pixel\n",
    "\n",
    "    def add_metadata(self, tab):\n",
    "        raise NotImplementedError()\n",
    "        # # Meta-data:\n",
    "        # tab.meta['fileid'] = fileid\n",
    "        # tab.meta['target_name'] = target_name\n",
    "        # tab.meta['version'] = __version__\n",
    "        # tab.meta['template'] = None if datafile.get('template') is None else datafile['template']['fileid']\n",
    "        # tab.meta['diffimg'] = None if datafile.get('diffimg') is None else datafile['diffimg']['fileid']\n",
    "        # tab.meta['photfilter'] = photfilter\n",
    "        # tab.meta['fwhm'] = fwhm * u.pixel\n",
    "        # tab.meta['pixel_scale'] = pixel_scale * u.arcsec / u.pixel\n",
    "        # tab.meta['seeing'] = (fwhm * pixel_scale) * u.arcsec\n",
    "        # tab.meta['obstime-bmjd'] = float(image.obstime.mjd)\n",
    "        # tab.meta['zp'] = zp_bs\n",
    "        # tab.meta['zp_error'] = zp_error_bs\n",
    "        # tab.meta['zp_diff'] = np.abs(zp_bs - zp)\n",
    "        # tab.meta['zp_error_weights'] = zp_error\n",
    "        # tab.meta['head_wcs'] = head_wcs  # TODO: Are these really useful?\n",
    "        # tab.meta['used_wcs'] = used_wcs  # TODO: Are these really useful?\n",
    "\n",
    "    @classmethod\n",
    "    def make_results_table(cls, ref_table: Table, apphot_tbl: Table, psfphot_tbl: Table, image: FlowsImage):\n",
    "        results_table = cls(ref_table)\n",
    "        if len(ref_table) - len(apphot_tbl) == 1:\n",
    "            results_table.add_row(0)\n",
    "\n",
    "        psfphot_tbl = ResultsTable.verify_uncertainty_column(psfphot_tbl)\n",
    "\n",
    "        results_table['flux_aperture'] = apphot_tbl['flux_aperture'] / image.exptime\n",
    "        results_table['flux_aperture_error'] = apphot_tbl['flux_aperture_error'] / image.exptime\n",
    "        results_table['flux_psf'] = psfphot_tbl['flux_fit'] / image.exptime\n",
    "        results_table['flux_psf_error'] = psfphot_tbl['flux_unc'] / image.exptime\n",
    "        results_table['pixel_column_psf_fit'] = psfphot_tbl['x_fit']\n",
    "        results_table['pixel_row_psf_fit'] = psfphot_tbl['y_fit']\n",
    "        results_table['pixel_column_psf_fit_error'] = psfphot_tbl['x_0_unc']\n",
    "        results_table['pixel_row_psf_fit_error'] = psfphot_tbl['y_0_unc']\n",
    "\n",
    "        return results_table\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_uncertainty_column(tab):\n",
    "        if \"flux_unc\" in tab.colnames:\n",
    "            return tab\n",
    "        tab['flux_unc'] = tab['flux_fit'] * 0.04  # Assume 4% errors\n",
    "        logger.warning(\"Flux uncertainty not found from PSF fit, assuming 4% error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f25362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .magnitudes import instrumental_mag (DONE ABOVE)\n",
    "#from .result_model import ResultsTable (DONE ABOVE)\n",
    "\n",
    "warnings.simplefilter('ignore', category=AstropyDeprecationWarning)\n",
    "from photutils import CircularAperture, CircularAnnulus, aperture_photometry  # noqa: E402\n",
    "from photutils.psf import EPSFFitter, BasicPSFPhotometry, DAOGroup, extract_stars  # noqa: E402\n",
    "from photutils.background import MedianBackground  # noqa: E402\n",
    "import photutils  # noqa: E402\n",
    "\n",
    "#from .reference_cleaning import References, ReferenceCleaner, InitGuess  # noqa: E402\n",
    "#from .plots import plt, plot_image  # noqa: E402\n",
    "#from .version import get_version  # noqa: E402\n",
    "#from .image import FlowsImage  # noqa: E402\n",
    "#from .coordinatematch import correct_wcs  # noqa: E402\n",
    "#from .epsfbuilder import FlowsEPSFBuilder, verify_epsf  # noqa: E402\n",
    "#from .fileio import DirectoryProtocol, IOManager  # noqa: E402\n",
    "#from .target import Target  # noqa: E402\n",
    "#from .background import FlowsBackground  # noqa: E402\n",
    "#from .utilities import create_logger  # noqa: E402\n",
    "\n",
    "### ALL THIS IS DONE BELOW###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa04ddd",
   "metadata": {},
   "source": [
    "# reference_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b418336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean bad source extraction, find and correct WCS.\n",
    "\n",
    ".. codeauthor:: Emir Karamehmetoglu <emir.k@phys.au.dk>\n",
    ".. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "\"\"\"\n",
    "from typing import Dict, Optional, TypeVar, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "import astroalign as aa\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import wcs\n",
    "from astropy.stats import sigma_clip, gaussian_fwhm_to_sigma\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.time import Time\n",
    "from astropy.utils.exceptions import ErfaWarning\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from copy import deepcopy\n",
    "from bottleneck import nanmedian, nansum, nanmean, replace\n",
    "from scipy.spatial import KDTree\n",
    "import pandas as pd  # TODO: Convert to pure numpy implementation\n",
    "import sep\n",
    "\n",
    "#from .image import FlowsImage\n",
    "#from .target import Target\n",
    "#from .utilities import create_logger\n",
    "logger = create_logger()\n",
    "\n",
    "RefTable = TypeVar('RefTable', Dict, ArrayLike, Table)\n",
    "\n",
    "\n",
    "class MinStarError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class References:\n",
    "    table: RefTable\n",
    "    coords: Optional[SkyCoord] = None\n",
    "    mask: Optional[np.ndarray] = None  # positive mask ie True where we want it.\n",
    "    xy: Optional[RefTable] = None\n",
    "\n",
    "    def replace_nans_pm(self) -> None:\n",
    "        replace(self.table['pm_ra'], np.NaN, 0.)\n",
    "        replace(self.table['pm_dec'], np.NaN, 0.)\n",
    "\n",
    "    def make_sky_coords(self, reference_time: float = 2015.5) -> None:\n",
    "        self.replace_nans_pm()\n",
    "        self.coords = SkyCoord(ra=self.table['ra'], dec=self.table['decl'], pm_ra_cosdec=self.table['pm_ra'],\n",
    "                        pm_dec=self.table['pm_dec'], unit='deg', frame='icrs',\n",
    "                        obstime=Time(reference_time, format='decimalyear'))\n",
    "\n",
    "    def propagate(self, obstime: Time) -> None:\n",
    "        if self.coords is None:\n",
    "            raise AttributeError(\"References.coords is not defined.\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", ErfaWarning)\n",
    "            self.coords = self.coords.apply_space_motion(new_obstime=obstime)\n",
    "\n",
    "    def copy(self) -> 'References':\n",
    "        return References(self.__dataclass_fields__)\n",
    "\n",
    "    @property\n",
    "    def masked(self, mask: Optional[np.ndarray] = None) -> \"References\":\n",
    "        if self.mask is None:\n",
    "            if mask is None:\n",
    "                raise AttributeError(\"No mask defined.\")\n",
    "            self.mask = mask\n",
    "\n",
    "        copy = self.copy()\n",
    "        for name in self.__dataclass_fields__:\n",
    "            if getattr(self, name) is not None:\n",
    "                setattr(copy, name, getattr(self, name)[self.mask])\n",
    "        return copy\n",
    "\n",
    "    def get_xy(self, img_wcs: wcs.WCS) -> None:\n",
    "        \"\"\"get pixel coordinates of reference stars\"\"\"\n",
    "        self.xy = img_wcs.all_world2pix(list(zip(self.coords.ra.deg, self.coords.dec.deg)), 0)\n",
    "\n",
    "    def make_pixel_columns(self, column_name: str ='pixel_column', row_name: str = 'pixel_row') -> None:\n",
    "        self.table[column_name], self.table[row_name] = list(map(np.array, zip(*self.xy)))\n",
    "\n",
    "    def _prepend_row(self, row: dict) -> None:\n",
    "        self.table.insert_row(0, row)\n",
    "\n",
    "    def add_target(self, target: Target, starid: int = 0) -> None:\n",
    "        self._prepend_row(target.output_dict(starid=starid))\n",
    "        if target.pixel_row and target.pixel_column:\n",
    "            self.xy = np.vstack(((target.pixel_column, target.pixel_row), self.xy))\n",
    "\n",
    "\n",
    "def use_sep(image: FlowsImage, tries: int = 5, thresh: float = 5.):\n",
    "\n",
    "    # Use sep to for soure extraction\n",
    "    sep_background = sep.Background(image.image, mask=image.mask)\n",
    "    try:\n",
    "        objects = sep.extract(image.image - sep_background, thresh=thresh, err=sep_background.globalrms,\n",
    "                              mask=image.mask, deblend_cont=0.1, minarea=9, clean_param=2.0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.warning(\"SEP failed, trying again...\")\n",
    "        if tries > 0:\n",
    "            thresh += 3\n",
    "            return use_sep(image, tries - 1, thresh * 2)\n",
    "        else:\n",
    "            raise e\n",
    "    sep_references = References(table=Table(objects))\n",
    "    sep_references.xy = sep_references.table[['x', 'y']]\n",
    "    sep_references.make_pixel_columns()\n",
    "    return sep_references\n",
    "\n",
    "def force_reject_g2d(xarray: ArrayLike, yarray: ArrayLike, image: Union[NDArray, np.ma.MaskedArray],\n",
    "                     rsq_min: float = 0.5, radius: float = 10, fwhm_guess: float = 6.0, fwhm_min: float = 3.5,\n",
    "                     fwhm_max: float = 18.0) -> Tuple[np.ma.MaskedArray, ...]:\n",
    "    \"\"\"\n",
    "    It takes a list of x and y coordinates, and a 2D image, and returns a list of x and y coordinates,\n",
    "    a list of r-squared values, and a boolean mask\n",
    "\n",
    "    :param xarray: x-coordinates of the stars\n",
    "    :type xarray: ArrayLike\n",
    "    :param yarray: y-coordinates of the stars\n",
    "    :type yarray: ArrayLike\n",
    "    :param image: the image to be processed\n",
    "    :type image: Union[NDArray, np.ma.MaskedArray]\n",
    "    :param rsq_min: The minimum r-squared value for a star to be considered good\n",
    "    :type rsq_min: float\n",
    "    :param radius: The radius of the box around the star to fit, defaults to 10\n",
    "    :type radius: float (optional)\n",
    "    :param fwhm_guess: The initial guess for the FWHM of the star\n",
    "    :type fwhm_guess: float\n",
    "    :param fwhm_min: The minimum FWHM allowed for a star\n",
    "    :type fwhm_min: float\n",
    "    :param fwhm_max: The maximum FWHM allowed for a star to be considered good\n",
    "    :type fwhm_max: float\n",
    "    :return: masked_fwhms, masked_xys, mask, masked_rsqs\n",
    "    \"\"\"\n",
    "    # Set up 2D Gaussian model for fitting to reference stars:\n",
    "    g2d = models.Gaussian2D(amplitude=1.0, x_mean=radius, y_mean=radius, x_stddev=fwhm_guess * gaussian_fwhm_to_sigma)\n",
    "    g2d.amplitude.bounds = (0.1, 2.0)\n",
    "    g2d.x_mean.bounds = (0.5 * radius, 1.5 * radius)\n",
    "    g2d.y_mean.bounds = (0.5 * radius, 1.5 * radius)\n",
    "    g2d.x_stddev.bounds = (fwhm_min * gaussian_fwhm_to_sigma, fwhm_max * gaussian_fwhm_to_sigma)\n",
    "    g2d.y_stddev.tied = lambda model: model.x_stddev\n",
    "    g2d.theta.fixed = True\n",
    "    gfitter = fitting.LevMarLSQFitter()\n",
    "\n",
    "    # Stars reject\n",
    "    N = len(xarray)\n",
    "    fwhms = np.full((N, 2), np.NaN)\n",
    "    xys = np.full((N, 2), np.NaN)\n",
    "    rsqs = np.full(N, np.NaN)\n",
    "    for i, (x, y) in enumerate(zip(xarray, yarray)):\n",
    "        x = int(np.round(x))\n",
    "        y = int(np.round(y))\n",
    "        xmin = max(x - radius, 0)\n",
    "        xmax = min(x + radius + 1, image.shape[1])\n",
    "        ymin = max(y - radius, 0)\n",
    "        ymax = min(y + radius + 1, image.shape[0])\n",
    "\n",
    "        curr_star = deepcopy(image[ymin:ymax, xmin:xmax])\n",
    "\n",
    "        edge = np.zeros_like(curr_star, dtype='bool')\n",
    "        edge[(0, -1), :] = True\n",
    "        edge[:, (0, -1)] = True\n",
    "        curr_star -= nanmedian(curr_star[edge])\n",
    "        curr_star /= np.nanmax(curr_star)\n",
    "\n",
    "        ypos, xpos = np.indices(curr_star.shape)\n",
    "        nan_filter = np.ones_like(curr_star, dtype='bool')\n",
    "        nan_filter = nan_filter & np.isfinite(curr_star)\n",
    "        if len(curr_star[nan_filter]) < 3:  # Not enough pixels to fit\n",
    "            logger.debug(f\"Not enough pixels to fit star, curr_star[nan_filter]:{curr_star[nan_filter]}\")\n",
    "            rsqs[i] = np.NaN\n",
    "            fwhms[i] = np.NaN\n",
    "            continue\n",
    "\n",
    "        gfit = gfitter(g2d, x=xpos[nan_filter], y=ypos[nan_filter], z=curr_star[nan_filter])\n",
    "\n",
    "        # Center\n",
    "        xys[i] = np.array([gfit.x_mean + x - radius, gfit.y_mean + y - radius], dtype='float64')\n",
    "\n",
    "        # Calculate rsq\n",
    "        sstot = nansum((curr_star - nanmean(curr_star)) ** 2)\n",
    "        sserr = nansum(gfitter.fit_info['fvec'] ** 2)\n",
    "        rsqs[i] = 0 if sstot == 0 else 1.0 - (sserr / sstot)\n",
    "\n",
    "        # FWHM\n",
    "        fwhms[i] = gfit.x_fwhm\n",
    "\n",
    "    masked_xys = np.ma.masked_array(xys, ~np.isfinite(xys))\n",
    "    masked_rsqs = np.ma.masked_array(rsqs, ~np.isfinite(rsqs))\n",
    "    mask = (masked_rsqs >= rsq_min) & (masked_rsqs < 1.0)  # Reject Rsq < rsq_min\n",
    "    # changed\n",
    "    # masked_xys = masked_xys[mask] # Clean extracted array.\n",
    "    # to\n",
    "    masked_xys.mask[~mask] = True\n",
    "    # don't know if it breaks anything, but it doesn't make sense if\n",
    "    # len(masked_xys) != len(masked_rsqs) FIXME\n",
    "    masked_fwhms = np.ma.masked_array(fwhms, ~np.isfinite(fwhms))\n",
    "\n",
    "    return masked_fwhms, masked_xys, mask.data, masked_rsqs\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def clean_with_rsq_and_get_fwhm(masked_fwhms, masked_rsqs, references, min_fwhm_references=2, min_references=6,\n",
    "                                rsq_min=0.15):\n",
    "    \"\"\"\n",
    "    Clean references and obtain fwhm using RSQ values.\n",
    "\n",
    "    Parameters:\n",
    "        masked_fwhms (np.ma.maskedarray): array of fwhms\n",
    "        masked_rsqs (np.ma.maskedarray): array of rsq values\n",
    "        references (astropy.table.Table): table of reference stars\n",
    "        min_fwhm_references: (Default 2) min stars to get a fwhm\n",
    "        min_references: (Default 6) min stars to aim for when cutting by R2\n",
    "        rsq_min: (Default 0.15) min rsq value\n",
    "\n",
    "    .. codeauthor:: Emir Karamehmetoglu <emir.k@phys.au.dk>\n",
    "    \"\"\"\n",
    "    min_references_now = min_references\n",
    "    rsqvals = np.arange(rsq_min, 0.95, 0.15)[::-1]\n",
    "    fwhm_found = False\n",
    "    min_references_achieved = False\n",
    "    fwhm = np.nan\n",
    "    # Clean based on R^2 Value\n",
    "    while not min_references_achieved:\n",
    "        for rsqval in rsqvals:\n",
    "            mask = (masked_rsqs >= rsqval) & (masked_rsqs < 1.0)\n",
    "            nreferences = np.sum(np.isfinite(masked_fwhms[mask]))\n",
    "            if nreferences >= min_fwhm_references:\n",
    "                _fwhms_cut_ = np.nanmean(sigma_clip(masked_fwhms[mask], maxiters=100, sigma=2.0))\n",
    "                if not fwhm_found:\n",
    "                    fwhm = _fwhms_cut_\n",
    "                    fwhm_found = True\n",
    "            if nreferences >= min_references_now:\n",
    "                references = references[mask]\n",
    "                min_references_achieved = True\n",
    "                break\n",
    "        if min_references_achieved:\n",
    "            break\n",
    "        min_references_now = min_references_now - 2\n",
    "        if (min_references_now < 2) and fwhm_found:\n",
    "            break\n",
    "        elif not fwhm_found:\n",
    "            raise RuntimeError(\"Could not estimate FWHM\")\n",
    "\n",
    "    if np.isnan(fwhm):\n",
    "        raise RuntimeError(\"Could not estimate FWHM\")\n",
    "\n",
    "    # if minimum references not found, then take what we can get with even a weaker cut.\n",
    "    # TODO: Is this right, or should we grab rsq_min (or even weaker?)\n",
    "    min_references_now = min_references - 2\n",
    "    while not min_references_achieved:\n",
    "        mask = (masked_rsqs >= rsq_min) & (masked_rsqs < 1.0)\n",
    "        nreferences = np.sum(np.isfinite(masked_fwhms[mask]))\n",
    "        if nreferences >= min_references_now:\n",
    "            references = references[mask]\n",
    "            min_references_achieved = True\n",
    "        rsq_min = rsq_min - 0.07\n",
    "        min_references_now = min_references_now - 1\n",
    "\n",
    "    # Check len of references as this is a destructive cleaning.\n",
    "    # if len(references) == 2: logger.info('2 reference stars remaining, check WCS and image quality')\n",
    "    if len(references) < 2:\n",
    "        raise MinStarError(f\"{len(references)} References remaining; could not estimate fwhm.\")\n",
    "    return fwhm, references\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def mkposxy(posx, posy):\n",
    "    '''Make 2D np array for astroalign'''\n",
    "    img_posxy = np.array([[x, y] for x, y in zip(posx, posy)], dtype=\"float64\")\n",
    "    return img_posxy\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def try_transform(source, target, pixeltol=2, nnearest=5, max_stars=50):\n",
    "    aa.NUM_NEAREST_NEIGHBORS = nnearest\n",
    "    aa.PIXEL_TOL = pixeltol\n",
    "    transform, (sourcestars, targetstars) = aa.find_transform(source, target, max_control_points=max_stars)\n",
    "    return sourcestars, targetstars\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def try_astroalign(source, target, pixeltol=2, nnearest=5, max_stars_n=50):\n",
    "    # Get indexes of matched stars\n",
    "    success = False\n",
    "    try:\n",
    "        source_stars, target_stars = try_transform(source, target, pixeltol=pixeltol, nnearest=nnearest,\n",
    "                                                   max_stars=max_stars_n)\n",
    "        source_ind = np.argwhere(np.in1d(source, source_stars)[::2]).flatten()\n",
    "        target_ind = np.argwhere(np.in1d(target, target_stars)[::2]).flatten()\n",
    "        success = True\n",
    "    except aa.MaxIterError:\n",
    "        source_ind, target_ind = 'None', 'None'\n",
    "    return source_ind, target_ind, success\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def min_to_max_astroalign(source, target, fwhm=5, fwhm_min=1, fwhm_max=4, knn_min=5, knn_max=20, max_stars=100,\n",
    "                          min_matches=3):\n",
    "    \"\"\"Try to find matches using astroalign asterisms by stepping through some parameters.\"\"\"\n",
    "    # Set max_control_points par based on number of stars and max_stars.\n",
    "    nstars = max(len(source), len(source))\n",
    "    if max_stars >= nstars:\n",
    "        max_stars_list = 'None'\n",
    "    else:\n",
    "        if max_stars > 60:\n",
    "            max_stars_list = (max_stars, 50, 4, 3)\n",
    "        else:\n",
    "            max_stars_list = (max_stars, 6, 4, 3)\n",
    "\n",
    "    # Create max_stars step-through list if not given\n",
    "    if max_stars_list == 'None':\n",
    "        if nstars > 6:\n",
    "            max_stars_list = (nstars, 5, 3)\n",
    "        elif nstars > 3:\n",
    "            max_stars_list = (nstars, 3)\n",
    "\n",
    "    pixeltols = np.linspace(int(fwhm * fwhm_min), int(fwhm * fwhm_max), 4, dtype=int)\n",
    "    nearest_neighbors = np.linspace(knn_min, min(knn_max, nstars), 4, dtype=int)\n",
    "\n",
    "    success = False\n",
    "    for max_stars_n in max_stars_list:\n",
    "        for pixeltol in pixeltols:\n",
    "            for nnearest in nearest_neighbors:\n",
    "                source_ind, target_ind, success = try_astroalign(source, target, pixeltol=pixeltol, nnearest=nnearest,\n",
    "                                                                 max_stars_n=max_stars_n)\n",
    "                if success:\n",
    "                    if len(source_ind) >= min_matches:\n",
    "                        return source_ind, target_ind, success\n",
    "                    else:\n",
    "                        success = False\n",
    "    return 'None', 'None', success\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def kdtree(source, target, fwhm=5, fwhm_max=4, min_matches=3):\n",
    "    '''Use KDTree to get nearest neighbor matches within fwhm_max*fwhm distance'''\n",
    "\n",
    "    # Use KDTree to rapidly efficiently query nearest neighbors\n",
    "\n",
    "    tt = KDTree(target)\n",
    "    st = KDTree(source)\n",
    "    matches_list = st.query_ball_tree(tt, r=fwhm * fwhm_max)\n",
    "\n",
    "    # indx = []\n",
    "    targets = []\n",
    "    sources = []\n",
    "    for j, (sstar, match) in enumerate(zip(source, matches_list)):\n",
    "        if np.array(target[match]).size != 0:\n",
    "            targets.append(match[0])\n",
    "            sources.append(j)\n",
    "    sources = np.array(sources, dtype=int)\n",
    "    targets = np.array(targets, dtype=int)\n",
    "\n",
    "    # Return indexes of matches\n",
    "    return sources, targets, len(sources) >= min_matches\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def get_new_wcs(extracted_ind, extracted_stars, clean_references, ref_ind, obstime, rakey='ra_obs', deckey='decl_obs'):\n",
    "    targets = (extracted_stars[extracted_ind][:, 0], extracted_stars[extracted_ind][:, 1])\n",
    "\n",
    "    c = SkyCoord(ra=clean_references[rakey][ref_ind], dec=clean_references[deckey][ref_ind], frame='icrs',\n",
    "                 obstime=obstime)\n",
    "    return wcs.utils.fit_wcs_from_points(targets, c)\n",
    "\n",
    "\n",
    "def make_rsq_mask(masked_rsqs: np.ma.MaskedArray) -> np.ndarray:\n",
    "    # Switching to pandas for easier selection\n",
    "    df = pd.DataFrame(masked_rsqs, columns=['rsq'])\n",
    "    return df.sort_values('rsq', ascending=False).dropna().index.values\n",
    "\n",
    "\n",
    "def get_clean_references(reference_table: RefTable, masked_rsqs: np.ma.MaskedArray, min_references_ideal: int = 6,\n",
    "                         min_references_abs: int = 3, rsq_min: float = 0.15, rsq_ideal: float = 0.5,\n",
    "                         keep_max: int = 100, rescue_bad: bool = True) -> Tuple[RefTable, np.ndarray]:\n",
    "    # Greedy first try\n",
    "    mask = (masked_rsqs >= rsq_ideal) & (masked_rsqs < 1.0)\n",
    "    mask = ~mask.data | mask.mask  # masked out of range values OR non-finite values\n",
    "    masked_rsqs.mask = mask\n",
    "    rsq_mask_index = make_rsq_mask(masked_rsqs)[:keep_max]\n",
    "    if len(rsq_mask_index) >= min_references_ideal:\n",
    "        return reference_table[rsq_mask_index], rsq_mask_index\n",
    "\n",
    "    # Desperate second try\n",
    "    mask = (masked_rsqs >= rsq_min) & (masked_rsqs < 1.0)\n",
    "    mask = ~mask.data | mask.mask\n",
    "    masked_rsqs.mask = mask\n",
    "    rsq_mask_index = make_rsq_mask(masked_rsqs)[:min_references_ideal]\n",
    "    if len(rsq_mask_index) >= min_references_abs:\n",
    "        return reference_table[rsq_mask_index], rsq_mask_index\n",
    "    if not rescue_bad:\n",
    "        raise MinStarError(f'Less than {min_references_abs} clean stars and rescue_bad = False')\n",
    "\n",
    "    # Extremely desperate last ditch attempt i.e. \"rescue bad\"\n",
    "    mask = (masked_rsqs >= 0.02) & (masked_rsqs < 1.0)\n",
    "    mask = ~mask.data | mask.mask\n",
    "    masked_rsqs.mask = mask\n",
    "    rsq_mask_index = make_rsq_mask(masked_rsqs)[:min_references_ideal]\n",
    "    if len(rsq_mask_index) < 2:\n",
    "        raise MinStarError('Less than 2 clean stars.')\n",
    "    return reference_table[rsq_mask_index], rsq_mask_index  # Return if len >= 2\n",
    "\n",
    "\n",
    "class ReferenceCleaner:\n",
    "\n",
    "    def __init__(self, image: FlowsImage, references: References, rsq_min: float = 0.3,\n",
    "                 min_references_ideal: int = 6, min_references_abs: int = 3):\n",
    "        self.image = image\n",
    "        self.references = references\n",
    "        self.rsq_min = rsq_min\n",
    "        self.min_references_ideal = min_references_ideal\n",
    "        self.min_references_abs = min_references_abs\n",
    "        self.gaussian_xys: Optional[np.ndarray] = None  # gaussian pixel positions\n",
    "\n",
    "    def _clean_extracted_stars(self, x: Optional[ArrayLike] = None,\n",
    "                               y: Optional[ArrayLike] = None) -> Tuple[np.ma.MaskedArray, ...]:\n",
    "        \"\"\"\n",
    "        Clean extracted stars.\n",
    "        :return: Tuple of masked_fwhms, masked_ref_xys, rsq_mask, masked_rsqs\n",
    "        \"\"\"\n",
    "        # use instrument_defaults for initial guess of FWHM\n",
    "        radius = self.image.instrument_defaults.radius\n",
    "        fwhm_guess = self.image.instrument_defaults.fwhm\n",
    "        fwhm_min = self.image.instrument_defaults.fwhm_min\n",
    "        fwhm_max = self.image.instrument_defaults.fwhm_max\n",
    "        useimage = self.image.subclean if self.image.subclean is not None else self.image.clean\n",
    "\n",
    "        # Clean the references\n",
    "        x = x if x is not None else self.references.table['pixel_column']\n",
    "        y = y if y is not None else self.references.table['pixel_row']\n",
    "        return force_reject_g2d(x, y, useimage, radius=radius, fwhm_guess=fwhm_guess, rsq_min=self.rsq_min,\n",
    "                                fwhm_max=fwhm_max, fwhm_min=fwhm_min)\n",
    "\n",
    "    def set_gaussian_xys(self, masked_ref_xys: np.ma.MaskedArray, old_references: RefTable,\n",
    "                         new_references: RefTable) -> None:\n",
    "        xy = [tuple(masked_ref_xys[old_references['starid'] == ref['starid']].data[0]) for ref in new_references]\n",
    "        self.gaussian_xys = np.array(xy)\n",
    "\n",
    "    def clean_references(self, references: References = None) -> Tuple[References, float]:\n",
    "        if references is None:\n",
    "            references = self.references\n",
    "\n",
    "        # Clean the references\n",
    "        masked_fwhms, masked_ref_xys, rsq_mask, masked_rsqs = self._clean_extracted_stars(\n",
    "            references.table['pixel_column'],\n",
    "            references.table['pixel_row'])\n",
    "\n",
    "\n",
    "        # Use R^2 to more robustly determine initial FWHM guess.\n",
    "        # This cleaning is good when we have FEW references.\n",
    "        fwhm, fwhm_clean_references = clean_with_rsq_and_get_fwhm(\n",
    "            masked_fwhms, masked_rsqs, references.table, min_fwhm_references=2,\n",
    "            min_references=self.min_references_abs, rsq_min=self.rsq_min)\n",
    "        logger.info('Initial FWHM guess is %f pixels', fwhm)\n",
    "\n",
    "        # Final clean of wcs corrected references\n",
    "        logger.info(\"Number of references before final cleaning: %d\", len(references.table))\n",
    "        logger.debug('Masked R^2 values: %s', masked_rsqs[rsq_mask].data)\n",
    "        # Get references cleaned and ordered by R^2:\n",
    "        ordered_cleaned_references, order_index = get_clean_references(references.table, masked_rsqs, rsq_ideal=0.8)\n",
    "        ordered_coords = None if references.coords is None else references.coords[order_index]\n",
    "        ordered_xy = None if references.xy is None else references.xy[order_index]\n",
    "        ordered_cleaned_references = References(table=ordered_cleaned_references, coords=ordered_coords, xy=ordered_xy)\n",
    "        logger.info(\"Number of references after final cleaning: %d\", len(ordered_cleaned_references.table))\n",
    "\n",
    "        # Save Gaussian XY positions before returning\n",
    "        self.set_gaussian_xys(masked_ref_xys, references.table, ordered_cleaned_references.table)\n",
    "        return ordered_cleaned_references, fwhm\n",
    "\n",
    "    def make_sep_clean_references(self) -> References:\n",
    "        \"\"\"\n",
    "        Make a clean reference catalog using SExtractor.\n",
    "        \"\"\"\n",
    "        image = self.image\n",
    "        # Get the SExtractor references from the image\n",
    "        sep_references = use_sep(image)\n",
    "\n",
    "        # Clean extracted stars\n",
    "        _, masked_sep_xy, sep_mask, masked_sep_rsqs = self._clean_extracted_stars(\n",
    "            sep_references.table['x'], sep_references.table['y'])\n",
    "\n",
    "        sep_references.mask = sep_mask\n",
    "        return sep_references.masked\n",
    "\n",
    "    def mask_edge_and_target(self, target_coords: SkyCoord, hsize: int = 10,\n",
    "                         target_distance_lim: u.quantity.Quantity = 10 * u.arcsec) -> References:\n",
    "        \"\"\"\n",
    "        Clean the references by removing references that are too close to the target.\n",
    "        \"\"\"\n",
    "        image_shape = self.image.shape\n",
    "\n",
    "        # Make mask\n",
    "        mask = (target_coords.separation(self.references.coords) > target_distance_lim) & (\n",
    "            self.references.table['pixel_column'] > hsize) & (self.references.table['pixel_column'] < (image_shape[1] - 1 - hsize)) & (\n",
    "            self.references.table['pixel_row'] > hsize) & (self.references.table['pixel_row'] < (image_shape[0] - 1 - hsize))\n",
    "        self.references.mask = mask\n",
    "\n",
    "        # Make new clean references\n",
    "        return self.references.masked\n",
    "        # return References(table=self.references.masked, mask=mask,\n",
    "        #                  coords=self.references.coords[mask],\n",
    "        #                  xy=self.references.xy[mask])\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InitGuess:\n",
    "    clean_references: References\n",
    "    target_row: int = 0\n",
    "    diff_row: Optional[int] = None\n",
    "\n",
    "    @property\n",
    "    def init_guess_full(self) -> Table:\n",
    "        return Table(self.clean_references.xy, names=['x_0', 'y_0'])\n",
    "\n",
    "    @property\n",
    "    def init_guess_target(self) -> Table:\n",
    "        return Table(self.init_guess_full[self.target_row])\n",
    "\n",
    "    @property\n",
    "    def init_guess_diff(self) -> Table:\n",
    "        if self.diff_row is None:\n",
    "            raise ValueError('`diff_row` is None, I cannot calculate the initial guesses for the difference image row.')\n",
    "        return Table(self.init_guess_full[self.diff_row])\n",
    "\n",
    "    @property\n",
    "    def init_guess_references(self) -> Table:\n",
    "        ref_begin = max(self.diff_row, self.target_row) + 1 if self.diff_row is not None else self.target_row + 1\n",
    "        return self.init_guess_full[ref_begin:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d314",
   "metadata": {},
   "source": [
    "# plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea710373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting utilities.\n",
    "\n",
    ".. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "\"\"\"\n",
    "import logging\n",
    "import copy\n",
    "import numpy as np\n",
    "from bottleneck import allnan\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import astropy.visualization as viz\n",
    "#from .utilities import create_logger\n",
    "\n",
    "logger = create_logger()\n",
    "# Change to a non-GUI backend since this\n",
    "# should be able to run on a cluster:\n",
    "plt.switch_backend('Agg')\n",
    "\n",
    "# Change the fonts used in plots:\n",
    "# TODO: Use stylesheets instead of overwriting defaults here\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def plots_interactive(backend=('Qt5Agg', 'MacOSX', 'Qt4Agg', 'Qt5Cairo', 'TkAgg')):\n",
    "    \"\"\"\n",
    "    Change plotting to using an interactive backend.\n",
    "\n",
    "    Parameters:\n",
    "        backend (str or list): Backend to change to. If not provided, will try different\n",
    "            interactive backends and use the first one that works.\n",
    "\n",
    "    .. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "    \"\"\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.debug(\"Valid interactive backends: %s\", matplotlib.rcsetup.interactive_bk)\n",
    "\n",
    "    if isinstance(backend, str):\n",
    "        backend = [backend]\n",
    "\n",
    "    for bckend in backend:\n",
    "        if bckend not in matplotlib.rcsetup.interactive_bk:\n",
    "            logger.warning(\"Interactive backend '%s' is not found\", bckend)\n",
    "            continue\n",
    "\n",
    "        # Try to change the backend, and catch errors\n",
    "        # it it didn't work:\n",
    "        try:\n",
    "            plt.switch_backend(bckend)\n",
    "        except (ModuleNotFoundError, ImportError):\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def plots_noninteractive():\n",
    "    \"\"\"\n",
    "    Change plotting to using a non-interactive backend, which can e.g. be used on a cluster.\n",
    "    Will set backend to 'Agg'.\n",
    "\n",
    "    .. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "    \"\"\"\n",
    "    plt.switch_backend('Agg')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def plot_image(image, ax=None, scale='log', cmap=None, origin='lower', xlabel=None, ylabel=None, cbar=None,\n",
    "               clabel='Flux ($e^{-}s^{-1}$)', cbar_ticks=None, cbar_ticklabels=None, cbar_pad=None, cbar_size='4%',\n",
    "               title=None, percentile=95.0, vmin=None, vmax=None, offset_axes=None, color_bad='k', **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function to plot a 2D image.\n",
    "\n",
    "    Parameters:\n",
    "        image (2d array): Image data.\n",
    "        ax (matplotlib.pyplot.axes, optional): Axes in which to plot.\n",
    "            Default (None) is to use current active axes.\n",
    "        scale (str or :py:class:`astropy.visualization.ImageNormalize` object, optional):\n",
    "            Normalization used to stretch the colormap.\n",
    "            Options: ``'linear'``, ``'sqrt'``, ``'log'``, ``'asinh'``, ``'histeq'``, ``'sinh'``\n",
    "            and ``'squared'``.\n",
    "            Can also be a :py:class:`astropy.visualization.ImageNormalize` object.\n",
    "            Default is ``'log'``.\n",
    "        origin (str, optional): The origin of the coordinate system.\n",
    "        xlabel (str, optional): Label for the x-axis.\n",
    "        ylabel (str, optional): Label for the y-axis.\n",
    "        cbar (string, optional): Location of color bar.\n",
    "            Choises are ``'right'``, ``'left'``, ``'top'``, ``'bottom'``.\n",
    "            Default is not to create colorbar.\n",
    "        clabel (str, optional): Label for the color bar.\n",
    "        cbar_size (float, optional): Fractional size of colorbar compared to axes. Default='4%'.\n",
    "        cbar_pad (float, optional): Padding between axes and colorbar.\n",
    "        title (str or None, optional): Title for the plot.\n",
    "        percentile (float, optional): The fraction of pixels to keep in color-trim.\n",
    "            If single float given, the same fraction of pixels is eliminated from both ends.\n",
    "            If tuple of two floats is given, the two are used as the percentiles.\n",
    "            Default=95.\n",
    "        cmap (matplotlib colormap, optional): Colormap to use. Default is the ``Blues`` colormap.\n",
    "        vmin (float, optional): Lower limit to use for colormap.\n",
    "        vmax (float, optional): Upper limit to use for colormap.\n",
    "        color_bad (str, optional): Color to apply to bad pixels (NaN). Default is black.\n",
    "        kwargs (dict, optional): Keyword arguments to be passed to :py:func:`matplotlib.pyplot.imshow`.\n",
    "\n",
    "    Returns:\n",
    "        :py:class:`matplotlib.image.AxesImage`: Image from returned\n",
    "            by :py:func:`matplotlib.pyplot.imshow`.\n",
    "\n",
    "    .. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "    \"\"\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Backward compatible settings:\n",
    "    make_cbar = kwargs.pop('make_cbar', None)\n",
    "    # noinspection PyUnreachableCode\n",
    "    if make_cbar:\n",
    "        raise FutureWarning(\"'make_cbar' is deprecated. Use 'cbar' instead.\")\n",
    "        if not cbar:\n",
    "            cbar = make_cbar\n",
    "\n",
    "    # Special treatment for boolean arrays:\n",
    "    if isinstance(image, np.ndarray) and image.dtype == 'bool':\n",
    "        if vmin is None: vmin = 0\n",
    "        if vmax is None: vmax = 1\n",
    "        if cbar_ticks is None: cbar_ticks = [0, 1]\n",
    "        if cbar_ticklabels is None: cbar_ticklabels = ['False', 'True']\n",
    "\n",
    "    # Calculate limits of color scaling:\n",
    "    interval = None\n",
    "    if vmin is None or vmax is None:\n",
    "        if allnan(image):\n",
    "            logger.warning(\"Image is all NaN\")\n",
    "            vmin = 0\n",
    "            vmax = 1\n",
    "            if cbar_ticks is None:\n",
    "                cbar_ticks = []\n",
    "            if cbar_ticklabels is None:\n",
    "                cbar_ticklabels = []\n",
    "        elif isinstance(percentile, (list, tuple, np.ndarray)):\n",
    "            interval = viz.AsymmetricPercentileInterval(percentile[0], percentile[1])\n",
    "        else:\n",
    "            interval = viz.PercentileInterval(percentile)\n",
    "\n",
    "    # Create ImageNormalize object with extracted limits:\n",
    "    if scale in ('log', 'linear', 'sqrt', 'asinh', 'histeq', 'sinh', 'squared'):\n",
    "        if scale == 'log':\n",
    "            stretch = viz.LogStretch()\n",
    "        elif scale == 'linear':\n",
    "            stretch = viz.LinearStretch()\n",
    "        elif scale == 'sqrt':\n",
    "            stretch = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b036ad",
   "metadata": {},
   "source": [
    "# version.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94070b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1-v1.0.0.post30+git657a5c4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get version identification from git\n",
    "\n",
    "If the script is located within an active git repository,\n",
    "git-describe is used to get the version information.\n",
    "If this is not a git repository, then it is reasonable to\n",
    "assume that the version is not being incremented and the\n",
    "version returned will be the release version as read from\n",
    "the VERSION file, which holds the version information.\n",
    "\n",
    "The file VERSION will need to be changed by manually. This should be done\n",
    "before running git tag (set to the same as the version in the tag).\n",
    "\n",
    "Inspired by\n",
    "https://github.com/aebrahim/python-git-version\n",
    "\n",
    ".. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    "\"\"\"\n",
    "\n",
    "from subprocess import check_output, CalledProcessError\n",
    "from os import path, name, devnull, environ, listdir\n",
    "\n",
    "__all__ = (\"get_version\",)\n",
    "\n",
    "CURRENT_DIRECTORY = path.dirname(path.abspath(FILE))\n",
    "VERSION_FILE = path.join(CURRENT_DIRECTORY, '..', 'VERSION')\n",
    "\n",
    "# Find the \"git\" command to run depending on the OS:\n",
    "GIT_COMMAND = \"git\"\n",
    "if name == \"nt\":\n",
    "    def find_git_on_windows():\n",
    "        \"\"\"find the path to the git executable on windows\"\"\"\n",
    "        # first see if git is in the path\n",
    "        try:\n",
    "            check_output([\"where\", \"/Q\", \"git\"])\n",
    "            # if this command succeeded, git is in the path\n",
    "            return \"git\"\n",
    "        # catch the exception thrown if git was not found\n",
    "        except CalledProcessError:\n",
    "            pass\n",
    "        # There are several locations git.exe may be hiding\n",
    "        possible_locations = []\n",
    "        # look in program files for msysgit\n",
    "        if \"PROGRAMFILES(X86)\" in environ:\n",
    "            possible_locations.append(\"%s/Git/cmd/git.exe\" % environ[\"PROGRAMFILES(X86)\"])\n",
    "        if \"PROGRAMFILES\" in environ:\n",
    "            possible_locations.append(\"%s/Git/cmd/git.exe\" % environ[\"PROGRAMFILES\"])\n",
    "        # look for the github version of git\n",
    "        if \"LOCALAPPDATA\" in environ:\n",
    "            github_dir = \"%s/GitHub\" % environ[\"LOCALAPPDATA\"]\n",
    "            if path.isdir(github_dir):\n",
    "                for subdir in listdir(github_dir):\n",
    "                    if not subdir.startswith(\"PortableGit\"):\n",
    "                        continue\n",
    "                    possible_locations.append(\"%s/%s/bin/git.exe\" % (github_dir, subdir))\n",
    "        for possible_location in possible_locations:\n",
    "            if path.isfile(possible_location):\n",
    "                return possible_location\n",
    "        # git was not found\n",
    "        return \"git\"\n",
    "\n",
    "\n",
    "    GIT_COMMAND = find_git_on_windows()\n",
    "\n",
    "\n",
    "def call_git_describe(abbrev=7):\n",
    "    \"\"\"return the string output of git desribe\"\"\"\n",
    "    try:\n",
    "        with open(devnull, \"w\") as fnull:\n",
    "            arguments = [GIT_COMMAND, \"describe\", \"--tags\", \"--abbrev=%d\" % abbrev]\n",
    "            return check_output(arguments, cwd=CURRENT_DIRECTORY, stderr=fnull).decode(\"ascii\").strip()\n",
    "    except (OSError, CalledProcessError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_git_getbranch():\n",
    "    try:\n",
    "        with open(devnull, \"w\") as fnull:\n",
    "            arguments = [GIT_COMMAND, \"symbolic-ref\", \"--short\", \"HEAD\"]\n",
    "            return check_output(arguments, cwd=CURRENT_DIRECTORY, stderr=fnull).decode(\"ascii\").strip()\n",
    "    except (OSError, CalledProcessError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def format_git_describe(git_str, pep440=False):\n",
    "    \"\"\"format the result of calling 'git describe' as a python version\"\"\"\n",
    "    if git_str is None:\n",
    "        return None\n",
    "    if \"-\" not in git_str:  # currently at a tag\n",
    "        return git_str\n",
    "    else:\n",
    "        # formatted as version-N-githash\n",
    "        # want to convert to version.postN-githash\n",
    "        git_str = git_str.replace(\"-\", \".post\", 1)\n",
    "        if pep440:  # does not allow git hash afterwards\n",
    "            return git_str.split(\"-\")[0]\n",
    "        else:\n",
    "            return git_str.replace(\"-g\", \"+git\")\n",
    "\n",
    "\n",
    "def read_release_version():\n",
    "    \"\"\"Read version information from VERSION file\"\"\"\n",
    "    try:\n",
    "        with open(VERSION_FILE, \"r\") as infile:\n",
    "            version = str(infile.read().strip())\n",
    "        if len(version) == 0:\n",
    "            version = None\n",
    "        return version\n",
    "    except IOError:\n",
    "        return None\n",
    "\n",
    "def update_release_version():\n",
    "    \"\"\"Update VERSION file\"\"\"\n",
    "    version = get_version(pep440=True)\n",
    "    with open(VERSION_FILE, \"w\") as outfile:\n",
    "        outfile.write(version)\n",
    "\n",
    "def get_version(pep440=False, include_branch=True):\n",
    "    \"\"\"\n",
    "    Tracks the version number.\n",
    "\n",
    "    The file VERSION holds the version information. If this is not a git\n",
    "    repository, then it is reasonable to assume that the version is not\n",
    "    being incremented and the version returned will be the release version as\n",
    "    read from the file.\n",
    "\n",
    "    However, if the script is located within an active git repository,\n",
    "    git-describe is used to get the version information.\n",
    "\n",
    "    The file VERSION will need to be changed by manually. This should be done\n",
    "    before running git tag (set to the same as the version in the tag).\n",
    "\n",
    "    Parameters:\n",
    "        pep440 (bool): When True, this function returns a version string suitable for\n",
    "        a release as defined by PEP 440. When False, the githash (if\n",
    "        available) will be appended to the version string.\n",
    "\n",
    "    Returns:\n",
    "        string: Version sting.\n",
    "    \"\"\"\n",
    "\n",
    "    git_version = format_git_describe(call_git_describe(), pep440=pep440)\n",
    "    if git_version is None:  # not a git repository\n",
    "        return read_release_version()\n",
    "\n",
    "    if include_branch:\n",
    "        git_branch = call_git_getbranch()\n",
    "        if git_branch is not None:\n",
    "            git_version = git_branch + '-' + git_version\n",
    "\n",
    "    return git_version\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(get_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227e640",
   "metadata": {},
   "source": [
    "# wcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7026de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WCS tools.\n",
    "\n",
    ".. codeauthor:: Simon Holmbo <simonholmbo@phys.au.dk>\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import astropy.wcs\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "class WCS2:\n",
    "    \"\"\"\n",
    "    Manipulate WCS solution.\n",
    "\n",
    "    Initialize\n",
    "    ----------\n",
    "    wcs = WCS2(x, y, ra, dec, scale, mirror, angle)\n",
    "    wcs = WCS2.from_matrix(x, y, ra, dec, matrix)\n",
    "    wcs = WCS2.from_points(list(zip(x, y)), list(zip(ra, dec)))\n",
    "    wcs = WCS2.from_astropy_wcs(astropy.wcs.WCS())\n",
    "\n",
    "    ra, dec and angle should be in degrees\n",
    "    scale should be in arcsec/pixel\n",
    "    matrix should be the PC or CD matrix\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Adjust x, y offset:\n",
    "    wcs.x += delta_x\n",
    "    wcs.y += delta_y\n",
    "\n",
    "    Get scale and angle:\n",
    "    print(wcs.scale, wcs.angle)\n",
    "\n",
    "    Change an astropy.wcs.WCS (wcs) angle\n",
    "    wcs = WCS2(wcs)(angle=new_angle).astropy_wcs\n",
    "\n",
    "    Adjust solution with points\n",
    "    wcs.adjust_with_points(list(zip(x, y)), list(zip(ra, dec)))\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def __init__(self, x, y, ra, dec, scale, mirror, angle):\n",
    "        self.x, self.y = x, y\n",
    "        self.ra, self.dec = ra, dec\n",
    "        self.scale = scale\n",
    "        self.mirror = mirror\n",
    "        self.angle = angle\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @classmethod\n",
    "    def from_matrix(cls, x, y, ra, dec, matrix):\n",
    "        '''Initiate the class with a matrix.'''\n",
    "\n",
    "        assert np.shape(matrix) == (2, 2), 'Matrix must be 2x2'\n",
    "\n",
    "        scale, mirror, angle = cls._decompose_matrix(matrix)\n",
    "\n",
    "        return cls(x, y, ra, dec, scale, mirror, angle)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @classmethod\n",
    "    def from_points(cls, xy, rd):\n",
    "        \"\"\"Initiate the class with at least pixel + sky coordinates.\"\"\"\n",
    "\n",
    "        assert np.shape(xy) == np.shape(rd) == (len(xy), 2) and len(\n",
    "            xy) > 2, 'Arguments must be lists of at least 3 sets of coordinates'\n",
    "\n",
    "        xy, rd = np.array(xy), np.array(rd)\n",
    "\n",
    "        x, y, ra, dec, matrix = cls._solve_from_points(xy, rd)\n",
    "        scale, mirror, angle = cls._decompose_matrix(matrix)\n",
    "\n",
    "        return cls(x, y, ra, dec, scale, mirror, angle)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @classmethod\n",
    "    def from_astropy_wcs(cls, astropy_wcs):\n",
    "        \"\"\"Initiate the class with an astropy.wcs.WCS object.\"\"\"\n",
    "\n",
    "        if not isinstance(astropy_wcs, astropy.wcs.WCS):\n",
    "            raise ValueError('Must be astropy.wcs.WCS')\n",
    "\n",
    "        (x, y), (ra, dec) = astropy_wcs.wcs.crpix, astropy_wcs.wcs.crval\n",
    "        scale, mirror, angle = cls._decompose_matrix(astropy_wcs.pixel_scale_matrix)\n",
    "\n",
    "        return cls(x, y, ra, dec, scale, mirror, angle)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def adjust_with_points(self, xy, rd):\n",
    "        \"\"\"\n",
    "        Adjust the WCS with pixel + sky coordinates.\n",
    "\n",
    "        If one set is given the change will be a simple offset.\n",
    "        If two sets are given the offset, angle and scale will be derived.\n",
    "        And if more sets are given a completely new solution will be found.\n",
    "        \"\"\"\n",
    "\n",
    "        assert np.shape(xy) == np.shape(rd) == (len(xy), 2), 'Arguments must be lists of sets of coordinates'\n",
    "\n",
    "        xy, rd = np.array(xy), np.array(rd)\n",
    "\n",
    "        self.x, self.y = xy.mean(axis=0)\n",
    "        self.ra, self.dec = rd.mean(axis=0)\n",
    "\n",
    "        A, b = xy - xy.mean(axis=0), rd - rd.mean(axis=0)\n",
    "        b[:, 0] *= np.cos(np.deg2rad(rd[:, 1]))\n",
    "\n",
    "        if len(xy) == 2:\n",
    "\n",
    "            M = np.diag([[-1, 1][self.mirror], 1])\n",
    "\n",
    "            def R(t):\n",
    "                return np.array([[np.cos(t), -np.sin(t)], [np.sin(t), np.cos(t)]])\n",
    "\n",
    "            def chi2(x):\n",
    "                return np.power(A.dot(x[1] / 60 / 60 * R(x[0]).dot(M).T) - b, 2).sum()\n",
    "\n",
    "            self.angle, self.scale = minimize(chi2, [self.angle, self.scale]).x\n",
    "\n",
    "        elif len(xy) > 2:\n",
    "            matrix = np.linalg.lstsq(A, b, rcond=None)[0].T\n",
    "            self.scale, self.mirror, self.angle = self._decompose_matrix(matrix)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def matrix(self):\n",
    "\n",
    "        scale = self.scale / 60 / 60\n",
    "        mirror = np.diag([[-1, 1][self.mirror], 1])\n",
    "        angle = np.deg2rad(self.angle)\n",
    "\n",
    "        matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "        return scale * matrix @ mirror\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def astropy_wcs(self):\n",
    "        wcs = astropy.wcs.WCS()\n",
    "        wcs.wcs.crpix = self.x, self.y\n",
    "        wcs.wcs.crval = self.ra, self.dec\n",
    "        wcs.wcs.pc = self.matrix\n",
    "        return wcs\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _solve_from_points(xy, rd):\n",
    "\n",
    "        (x, y), (ra, dec) = xy.mean(axis=0), rd.mean(axis=0)\n",
    "\n",
    "        A, b = xy - xy.mean(axis=0), rd - rd.mean(axis=0)\n",
    "        b[:, 0] *= np.cos(np.deg2rad(rd[:, 1]))\n",
    "\n",
    "        matrix = np.linalg.lstsq(A, b, rcond=None)[0].T\n",
    "\n",
    "        return x, y, ra, dec, matrix\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _decompose_matrix(matrix):\n",
    "\n",
    "        scale = np.sqrt(np.power(matrix, 2).sum() / 2) * 60 * 60\n",
    "\n",
    "        if np.argmax(np.power(matrix[0], 2)):\n",
    "            mirror = True if np.sign(matrix[0, 1]) != np.sign(matrix[1, 0]) else False\n",
    "        else:\n",
    "            mirror = True if np.sign(matrix[0, 0]) == np.sign(matrix[1, 1]) else False\n",
    "\n",
    "        matrix = matrix if mirror else matrix.dot(np.diag([-1, 1]))\n",
    "\n",
    "        matrix3d = np.eye(3)\n",
    "        matrix3d[:2, :2] = matrix / (scale / 60 / 60)\n",
    "        angle = Rotation.from_matrix(matrix3d).as_euler('xyz', degrees=True)[2]\n",
    "\n",
    "        return scale, mirror, angle\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def __setattr__(self, name, value):\n",
    "\n",
    "        if name == 'ra' and (value < 0 or value >= 360):\n",
    "            raise ValueError(\"0 <= R.A. < 360\")\n",
    "\n",
    "        elif name == 'dec' and (value < -180 or value > 180):\n",
    "            raise ValueError(\"-180 <= Dec. <= 180\")\n",
    "\n",
    "        elif name == 'scale' and value <= 0:\n",
    "            raise ValueError(\"Scale > 0\")\n",
    "\n",
    "        elif name == 'mirror' and not isinstance(value, bool):\n",
    "            raise ValueError('mirror must be boolean')\n",
    "\n",
    "        elif name == 'angle' and (value <= -180 or value > 180):\n",
    "            raise ValueError(\"-180 < Angle <= 180\")\n",
    "\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def __call__(self, **kwargs):\n",
    "        '''Make a copy with, or a copy with changes.'''\n",
    "\n",
    "        keys = ('x', 'y', 'ra', 'dec', 'scale', 'mirror', 'angle')\n",
    "\n",
    "        if not all(k in keys for k in kwargs):\n",
    "            raise ValueError('unknown argument(s)')\n",
    "\n",
    "        obj = deepcopy(self)\n",
    "        for k, v in kwargs.items():\n",
    "            obj.__setattr__(k, v)\n",
    "        return obj\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def __repr__(self):\n",
    "        ra, dec = self.astropy_wcs.wcs_pix2world([(0, 0)], 0)[0]\n",
    "        return f'WCS2(0, 0, {ra:.4f}, {dec:.4f}, {self.scale:.2f}, {self.mirror}, {self.angle:.2f})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332665f5",
   "metadata": {},
   "source": [
    "# coordinatematch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0263b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Match two sets of coordinates.\n",
    "\n",
    ".. codeauthor:: Simon Holmbo <simonholmbo@phys.au.dk>\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import count, islice, chain, product, zip_longest\n",
    "from astropy.coordinates.angle_utilities import angular_separation\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.wcs\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "from networkx import Graph, connected_components\n",
    "#from .wcs import WCS2\n",
    "#from flows.utilities import create_logger\n",
    "#from flows.image import FlowsImage\n",
    "#from flows import reference_cleaning as refclean\n",
    "#from flows.target import Target\n",
    "\n",
    "\n",
    "logger = create_logger()\n",
    "\n",
    "\n",
    "def correct_wcs(image: FlowsImage, references: refclean.References, target: Target,\n",
    "                timeout: float = np.inf) -> FlowsImage:\n",
    "    \"\"\"\n",
    "    Correct WCS of image to match the reference image.\n",
    "    \"\"\"\n",
    "    # Start pre-cleaning\n",
    "    sep_cleaner = refclean.ReferenceCleaner(image, references, rsq_min=0.3)\n",
    "\n",
    "    # Use Source Extractor to make clean references\n",
    "    sep_references_clean = sep_cleaner.make_sep_clean_references()\n",
    "    xy = np.lib.recfunctions.structured_to_unstructured(np.array(sep_references_clean.xy))\n",
    "    # Find WCS\n",
    "    logger.info(\"Finding new WCS solution...\")\n",
    "    head_wcs = str(WCS2.from_astropy_wcs(image.wcs))\n",
    "    logger.debug('Head WCS: %s', head_wcs)\n",
    "    # Solve for new WCS\n",
    "    cm = CoordinateMatch(\n",
    "        xy=xy,\n",
    "        rd=np.array(list(zip(references.coords.ra.deg, references.coords.dec.deg))),\n",
    "        xy_order=np.argsort(np.power(xy - np.array(image.shape[::-1]) / 2, 2).sum(axis=1)),\n",
    "        rd_order=np.argsort(target.coords.separation(references.coords)),\n",
    "        xy_nmax=100, rd_nmax=100, maximum_angle_distance=0.002)\n",
    "\n",
    "    try:\n",
    "        i_xy, i_rd = map(np.array, zip(*cm(5, 1.5, timeout=timeout)))\n",
    "    except TimeoutError:\n",
    "        logger.warning('TimeoutError: No new WCS solution found')\n",
    "    except StopIteration:\n",
    "        logger.warning('StopIterationError: No new WCS solution found')\n",
    "    else:\n",
    "        logger.info('Found new WCS')\n",
    "        image.wcs = astropy.wcs.utils.fit_wcs_from_points(np.array(list(zip(*cm.xy[i_xy]))),\n",
    "                                                          SkyCoord(*map(list, zip(*cm.rd[i_rd])), unit='deg'))\n",
    "        del i_xy, i_rd\n",
    "\n",
    "    logger.debug(f'Used WCS: {WCS2.from_astropy_wcs(image.wcs)}')\n",
    "    return image\n",
    "\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "class CoordinateMatch(object):\n",
    "    def __init__(self, xy, rd, xy_order=None, rd_order=None, xy_nmax=None, rd_nmax=None, n_triangle_packages=10,\n",
    "                 triangle_package_size=10000, maximum_angle_distance=0.001, distance_factor=1):\n",
    "\n",
    "        self.xy, self.rd = np.array(xy), np.array(rd)\n",
    "\n",
    "        self._xy = xy - np.mean(xy, axis=0)\n",
    "        self._rd = rd - np.mean(rd, axis=0)\n",
    "        self._rd[:, 0] *= np.cos(np.deg2rad(self.rd[:, 1]))\n",
    "\n",
    "        xy_n, rd_n = min(xy_nmax, len(xy)), min(rd_nmax, len(rd))\n",
    "\n",
    "        self.i_xy = xy_order[:xy_n] if xy_order is not None else np.arange(xy_n)\n",
    "        self.i_rd = rd_order[:rd_n] if rd_order is not None else np.arange(rd_n)\n",
    "\n",
    "        self.n_triangle_packages = n_triangle_packages\n",
    "        self.triangle_package_size = triangle_package_size\n",
    "\n",
    "        self.maximum_angle_distance = maximum_angle_distance\n",
    "        self.distance_factor = distance_factor\n",
    "\n",
    "        self.triangle_package_generator = self._sorted_triangle_packages()\n",
    "\n",
    "        self.i_xy_triangles = list()\n",
    "        self.i_rd_triangles = list()\n",
    "        self.parameters = None\n",
    "        self.neighbours = Graph()\n",
    "\n",
    "        self.normalizations = type('Normalizations', (object,), dict(ra=0.0001, dec=0.0001, scale=0.002, angle=0.002))\n",
    "\n",
    "        self.bounds = type('Bounds', (object,),\n",
    "                           dict(xy=self.xy.mean(axis=0), rd=None, radius=None, scale=None, angle=None))\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def set_normalizations(self, ra=None, dec=None, scale=None, angle=None):\n",
    "        \"\"\"\n",
    "        Set normalization factors in the (ra, dec, scale, angle) space.\n",
    "\n",
    "        Defaults are:\n",
    "            ra = 0.0001 degrees\n",
    "            dec = 0.0001 degrees\n",
    "            scale = 0.002 log(arcsec/pixel)\n",
    "            angle = 0.002 radians\n",
    "        \"\"\"\n",
    "\n",
    "        if self.parameters is not None:\n",
    "            raise RuntimeError(\"can't change normalization after matching is started\")\n",
    "\n",
    "        # TODO: Dont use \"assert\" here - raise ValueError instead\n",
    "        assert ra is None or 0 < ra\n",
    "        assert dec is None or 0 < dec\n",
    "        assert scale is None or 0 < scale\n",
    "        assert angle is None or 0 < angle\n",
    "\n",
    "        self.normalizations.ra = ra if ra is not None else self.normalizations.ra\n",
    "        self.normalizations.dec = dec if dec is not None else self.normalizations.dec\n",
    "        self.normalizations.scale = scale if scale is not None else self.normalizations.scale\n",
    "        self.normalizations.angle = angle if ra is not None else self.normalizations.angle\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def set_bounds(self, x=None, y=None, ra=None, dec=None, radius=None, scale=None, angle=None):\n",
    "        \"\"\"\n",
    "        Set bounds for what are valid results.\n",
    "\n",
    "        Set x, y, ra, dec and radius to specify that the x, y coordinates must be no\n",
    "        further that the radius [degrees] away from the ra, dec coordinates.\n",
    "        Set upper and lower bounds on the scale [log(arcsec/pixel)] and/or the angle\n",
    "        [radians] if those are known, possibly from previous observations with the\n",
    "        same system.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.parameters is not None:\n",
    "            raise RuntimeError(\"can't change bounds after matching is started\")\n",
    "\n",
    "        if [x, y, ra, dec, radius].count(None) == 5:\n",
    "            # TODO: Dont use \"assert\" here - raise ValueError instead\n",
    "            assert 0 <= ra < 360\n",
    "            assert -180 <= dec <= 180\n",
    "            assert 0 < radius\n",
    "\n",
    "            self.bounds.xy = x, y\n",
    "            self.bounds.rd = ra, dec\n",
    "            self.bounds.radius = radius\n",
    "\n",
    "        elif [x, y, ra, dec, radius].count(None) > 0:\n",
    "            raise ValueError('x, y, ra, dec and radius must all be specified')\n",
    "\n",
    "        # TODO: Dont use \"assert\" here - raise ValueError instead\n",
    "        assert scale is None or 0 < scale[0] < scale[1]\n",
    "        assert angle is None or -np.pi <= angle[0] < angle[1] <= np.pi\n",
    "\n",
    "        self.bounds.scale = scale if scale is not None else self.bounds.scale\n",
    "        self.bounds.angle = angle if angle is not None else self.bounds.angle\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _sorted_triangles(self, pool):\n",
    "        for i, c in enumerate(pool):\n",
    "            for i, b in enumerate(pool[:i]):\n",
    "                for a in pool[:i]:\n",
    "                    yield a, b, c\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _sorted_product_pairs(self, p, q):\n",
    "        i_p = np.argsort(np.arange(len(p)))\n",
    "        i_q = np.argsort(np.arange(len(q)))\n",
    "        for _i_p, _i_q in sorted(product(i_p, i_q), key=lambda idxs: sum(idxs)):\n",
    "            yield p[_i_p], q[_i_q]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _sorted_triangle_packages(self):\n",
    "\n",
    "        i_xy_triangle_generator = self._sorted_triangles(self.i_xy)\n",
    "        i_rd_triangle_generator = self._sorted_triangles(self.i_rd)\n",
    "\n",
    "        i_xy_triangle_slice_generator = (tuple(islice(i_xy_triangle_generator, self.triangle_package_size)) for _ in\n",
    "                                         count())\n",
    "        i_rd_triangle_slice_generator = (list(islice(i_rd_triangle_generator, self.triangle_package_size)) for _ in\n",
    "                                         count())\n",
    "\n",
    "        for n in count(step=self.n_triangle_packages):\n",
    "\n",
    "            i_xy_triangle_slice = tuple(filter(None, islice(i_xy_triangle_slice_generator, self.n_triangle_packages)))\n",
    "            i_rd_triangle_slice = tuple(filter(None, islice(i_rd_triangle_slice_generator, self.n_triangle_packages)))\n",
    "\n",
    "            if not len(i_xy_triangle_slice) and not len(i_rd_triangle_slice):\n",
    "                return\n",
    "\n",
    "            i_xy_triangle_generator2 = self._sorted_triangles(self.i_xy)\n",
    "            i_rd_triangle_generator2 = self._sorted_triangles(self.i_rd)\n",
    "\n",
    "            i_xy_triangle_cum = filter(None,\n",
    "                                       (tuple(islice(i_xy_triangle_generator2, self.triangle_package_size)) for _ in\n",
    "                                        range(n)))\n",
    "            i_rd_triangle_cum = filter(None,\n",
    "                                       (tuple(islice(i_rd_triangle_generator2, self.triangle_package_size)) for _ in\n",
    "                                        range(n)))\n",
    "\n",
    "            for i_xy_triangles, i_rd_triangles in chain(filter(None, chain(*zip_longest(  # alternating chain\n",
    "                    product(i_xy_triangle_slice, i_rd_triangle_cum), product(i_xy_triangle_cum, i_rd_triangle_slice)))),\n",
    "                                                        self._sorted_product_pairs(i_xy_triangle_slice,\n",
    "                                                                                   i_rd_triangle_slice)):\n",
    "                yield np.array(i_xy_triangles), np.array(i_rd_triangles)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _get_triangle_angles(self, triangles):\n",
    "\n",
    "        sidelengths = np.sqrt(np.power(triangles[:, (1, 0, 0)] - triangles[:, (2, 2, 1)], 2).sum(axis=2))\n",
    "\n",
    "        # law of cosines\n",
    "        angles = np.power(sidelengths[:, ((1, 2), (0, 2), (0, 1))], 2).sum(axis=2)\n",
    "        angles -= np.power(sidelengths[:, (0, 1, 2)], 2)\n",
    "        angles /= 2 * sidelengths[:, ((1, 2), (0, 2), (0, 1))].prod(axis=2)\n",
    "\n",
    "        return np.arccos(angles)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _solve_for_matrices(self, xy_triangles, rd_triangles):\n",
    "\n",
    "        n = len(xy_triangles)\n",
    "\n",
    "        A = xy_triangles - np.mean(xy_triangles, axis=1).reshape(n, 1, 2)\n",
    "        b = rd_triangles - np.mean(rd_triangles, axis=1).reshape(n, 1, 2)\n",
    "\n",
    "        matrices = [np.linalg.lstsq(Ai, bi, rcond=None)[0].T for Ai, bi in zip(A, b)]\n",
    "\n",
    "        return np.array(matrices)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _extract_parameters(self, xy_triangles, rd_triangles, matrices):\n",
    "\n",
    "        parameters = []\n",
    "        for xy_com, rd_com, matrix in zip(xy_triangles.mean(axis=1), rd_triangles.mean(axis=1), matrices):\n",
    "            # com -> center-of-mass\n",
    "\n",
    "            cos_dec = np.cos(np.deg2rad(rd_com[1]))\n",
    "            coordinates = (self.bounds.xy - xy_com).dot(matrix)\n",
    "            coordinates = coordinates / np.array([cos_dec, 1]) + rd_com\n",
    "\n",
    "            wcs = WCS2.from_matrix(*xy_com, *rd_com, matrix)\n",
    "\n",
    "            parameters.append((*coordinates, np.log(wcs.scale), np.deg2rad(wcs.angle)))\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def _get_bounds_mask(self, parameters):\n",
    "\n",
    "        i = np.ones(len(parameters), dtype=bool)\n",
    "        parameters = np.array(parameters)\n",
    "\n",
    "        if self.bounds.radius is not None:\n",
    "            i *= angular_separation(*np.deg2rad(self.bounds.rd),\n",
    "                                    *zip(*np.deg2rad(parameters[:, (0, 1)]))) <= np.deg2rad(self.bounds.radius)\n",
    "\n",
    "        if self.bounds.scale is not None:\n",
    "            i *= self.bounds.scale[0] <= parameters[:, 2]\n",
    "            i *= parameters[:, 2] <= self.bounds.scale[1]\n",
    "\n",
    "        if self.bounds.angle is not None:\n",
    "            i *= self.bounds.angle[0] <= parameters[:, 3]\n",
    "            i *= parameters[:, 3] <= self.bounds.angle[1]\n",
    "\n",
    "        return i\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def __call__(self, minimum_matches=4, ratio_superiority=1, timeout=60):\n",
    "        \"\"\"\n",
    "        Start the alogrithm.\n",
    "\n",
    "        Can be run multiple times with different arguments to relax the\n",
    "        restrictions.\n",
    "\n",
    "        Example\n",
    "        --------\n",
    "        cm = CoordinateMatch(xy, rd)\n",
    "\n",
    "        lkwargs = [{\n",
    "            minimum_matches = 20,\n",
    "            ratio_superiority = 5,\n",
    "            timeout = 10\n",
    "        },{\n",
    "            timeout = 60\n",
    "        }\n",
    "\n",
    "        for i, kwargs in enumerate(lkwargs):\n",
    "            try:\n",
    "                i_xy, i_rd = cm(**kwargs)\n",
    "            except TimeoutError:\n",
    "                continue\n",
    "            except StopIteration:\n",
    "                print('Failed, no more stars.')\n",
    "            else:\n",
    "                print('Success with kwargs[%d].' % i)\n",
    "        else:\n",
    "            print('Failed, timeout.')\n",
    "        \"\"\"\n",
    "\n",
    "        self.parameters = list() if self.parameters is None else self.parameters\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        while time.time() - t0 < timeout:\n",
    "\n",
    "            # get triangles and derive angles\n",
    "\n",
    "            i_xy_triangles, i_rd_triangles = next(self.triangle_package_generator)\n",
    "\n",
    "            xy_angles = self._get_triangle_angles(self._xy[i_xy_triangles])\n",
    "            rd_angles = self._get_triangle_angles(self._rd[i_rd_triangles])\n",
    "\n",
    "            # sort triangle vertices based on angles\n",
    "\n",
    "            i = np.argsort(xy_angles, axis=1)\n",
    "            i_xy_triangles = np.take_along_axis(i_xy_triangles, i, axis=1)\n",
    "            xy_angles = np.take_along_axis(xy_angles, i, axis=1)\n",
    "\n",
    "            i = np.argsort(rd_angles, axis=1)\n",
    "            i_rd_triangles = np.take_along_axis(i_rd_triangles, i, axis=1)\n",
    "            rd_angles = np.take_along_axis(rd_angles, i, axis=1)\n",
    "\n",
    "            # match triangles\n",
    "            matches = KDTree(xy_angles).query_ball_tree(KDTree(rd_angles), r=self.maximum_angle_distance)\n",
    "            matches = np.array([(_i_xy, _i_rd) for _i_xy, _li_rd in enumerate(matches) for _i_rd in _li_rd])\n",
    "\n",
    "            if not len(matches):\n",
    "                continue\n",
    "\n",
    "            i_xy_triangles = list(i_xy_triangles[matches[:, 0]])\n",
    "            i_rd_triangles = list(i_rd_triangles[matches[:, 1]])\n",
    "\n",
    "            # get parameters of wcs solutions\n",
    "            matrices = self._solve_for_matrices(self._xy[np.array(i_xy_triangles)], self._rd[np.array(i_rd_triangles)])\n",
    "\n",
    "            parameters = self._extract_parameters(self.xy[np.array(i_xy_triangles)], self.rd[np.array(i_rd_triangles)],\n",
    "                                                  matrices)\n",
    "\n",
    "            # apply bounds if any\n",
    "            if any([self.bounds.radius, self.bounds.scale, self.bounds.angle]):\n",
    "                mask = self._get_bounds_mask(parameters)\n",
    "\n",
    "                i_xy_triangles = np.array(i_xy_triangles)[mask].tolist()\n",
    "                i_rd_triangles = np.array(i_rd_triangles)[mask].tolist()\n",
    "                parameters = np.array(parameters)[mask].tolist()\n",
    "\n",
    "            # normalize parameters\n",
    "            normalization = [getattr(self.normalizations, v) for v in ('ra', 'dec', 'scale', 'angle')]\n",
    "            normalization[0] *= np.cos(np.deg2rad(self.rd[:, 1].mean(axis=0)))\n",
    "            parameters = list(parameters / np.array(normalization))\n",
    "\n",
    "            # match parameters\n",
    "            neighbours = KDTree(parameters).query_ball_tree(KDTree(self.parameters + parameters),\n",
    "                                                            r=self.distance_factor)\n",
    "            neighbours = np.array([(i, j) for i, lj in enumerate(neighbours, len(self.parameters)) for j in lj])\n",
    "            neighbours = list(neighbours[(np.diff(neighbours, axis=1) < 0).flatten()])\n",
    "\n",
    "            if not len(neighbours):\n",
    "                continue\n",
    "\n",
    "            self.i_xy_triangles += i_xy_triangles\n",
    "            self.i_rd_triangles += i_rd_triangles\n",
    "            self.parameters += parameters\n",
    "            self.neighbours.add_edges_from(neighbours)\n",
    "\n",
    "            # get largest neighborhood\n",
    "            communities = list(connected_components(self.neighbours))\n",
    "            c1 = np.array(list(max(communities, key=len)))\n",
    "            i = np.unique(np.array(self.i_xy_triangles)[c1].flatten(), return_index=True)[1]\n",
    "\n",
    "            if ratio_superiority > 1 and len(communities) > 1:\n",
    "                communities.remove(set(c1))\n",
    "                c2 = np.array(list(max(communities, key=len)))\n",
    "                _i = np.unique(np.array(self.i_xy_triangles)[c2].flatten())\n",
    "\n",
    "                if len(i) / len(_i) < ratio_superiority:\n",
    "                    continue\n",
    "\n",
    "            if len(i) >= minimum_matches:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            raise TimeoutError\n",
    "\n",
    "        i_xy = np.array(self.i_xy_triangles)[c1].flatten()[i]\n",
    "        i_rd = np.array(self.i_rd_triangles)[c1].flatten()[i]\n",
    "\n",
    "        return list(zip(i_xy, i_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c6394",
   "metadata": {},
   "source": [
    "# epsfbuilder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7901c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Photutils hack for EPSF building\n",
    "\n",
    ".. codeauthor:: Simon Holmbo <simonholmbo@phys.au.dk>\n",
    "\"\"\"\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata, UnivariateSpline\n",
    "import photutils.psf\n",
    "from typing import List, Tuple\n",
    "#from flows.utilities import create_logger\n",
    "logger = create_logger()\n",
    "\n",
    "class FlowsEPSFBuilder(photutils.psf.EPSFBuilder):\n",
    "    def _create_initial_epsf(self, stars):\n",
    "        epsf = super()._create_initial_epsf(stars)\n",
    "        epsf.origin = None\n",
    "\n",
    "        X, Y = np.meshgrid(*map(np.arange, epsf.shape[::-1]))\n",
    "\n",
    "        X = X / epsf.oversampling[0] - epsf.x_origin\n",
    "        Y = Y / epsf.oversampling[1] - epsf.y_origin\n",
    "\n",
    "        self._epsf_xy_grid = X, Y\n",
    "\n",
    "        return epsf\n",
    "\n",
    "    def _resample_residual(self, star, epsf):\n",
    "        # max_dist = .5 / np.sqrt(np.sum(np.power(epsf.oversampling, 2)))\n",
    "\n",
    "        # star_points = list(zip(star._xidx_centered, star._yidx_centered))\n",
    "        # epsf_points = list(zip(*map(np.ravel, self._epsf_xy_grid)))\n",
    "\n",
    "        # star_tree = cKDTree(star_points)\n",
    "        # dd, ii = star_tree.query(epsf_points, distance_upper_bound=max_dist)\n",
    "        # mask = np.isfinite(dd)\n",
    "\n",
    "        # star_data = np.full_like(epsf.data, np.nan)\n",
    "        # star_data.ravel()[mask] = star._data_values_normalized[ii[mask]]\n",
    "\n",
    "        star_points = list(zip(star._xidx_centered, star._yidx_centered))\n",
    "        star_data = griddata(star_points, star._data_values_normalized, self._epsf_xy_grid)\n",
    "\n",
    "        return star_data - epsf._data\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        t0 = time.time()\n",
    "\n",
    "        epsf, stars = super().__call__(*args, **kwargs)\n",
    "\n",
    "        epsf.fit_info = dict(n_iter=len(self._epsf), max_iters=self.maxiters, time=time.time() - t0, )\n",
    "\n",
    "        return epsf, stars\n",
    "\n",
    "\n",
    "def verify_epsf(epsf: photutils.psf.EPSFModel) -> Tuple[bool, List[float]]:\n",
    "    fwhms = []\n",
    "    epsf_ok = True\n",
    "    for a in (0, 1):\n",
    "        # Collapse the PDF along this axis:\n",
    "        profile = epsf.data.sum(axis=a)\n",
    "        itop = profile.argmax()\n",
    "        poffset = profile[itop] / 2\n",
    "\n",
    "        # Run a spline through the points, but subtract half of the peak value, and find the roots:\n",
    "        # We have to use a cubic spline, since roots() is not supported for other splines\n",
    "        # for some reason\n",
    "        profile_intp = UnivariateSpline(np.arange(0, len(profile)), profile - poffset, k=3, s=0, ext=3)\n",
    "        lr = profile_intp.roots()\n",
    "\n",
    "        # Do some sanity checks on the ePSF:\n",
    "        # It should pass 50% exactly twice and have the maximum inside that region.\n",
    "        # I.e. it should be a single gaussian-like peak\n",
    "        if len(lr) != 2 or itop < lr[0] or itop > lr[1]:\n",
    "            logger.error(f\"EPSF is not a single gaussian-like peak along axis {a}\")\n",
    "            epsf_ok = False\n",
    "        else:\n",
    "            axis_fwhm = lr[1] - lr[0]\n",
    "            fwhms.append(axis_fwhm)\n",
    "    return epsf_ok, fwhms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b40ab",
   "metadata": {},
   "source": [
    "# filoio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ca111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Protocol, Dict, TypeVar, Union\n",
    "from configparser import ConfigParser\n",
    "from bottleneck import allnan\n",
    "from tendrils import api, utils\n",
    "#from .load_image import load_image\n",
    "#from .utilities import create_logger\n",
    "#from .target import Target\n",
    "#from .image import FlowsImage\n",
    "#from . import reference_cleaning as refclean\n",
    "#from .filters import get_reference_filter\n",
    "logger = create_logger()\n",
    "\n",
    "DataFileType = TypeVar(\"DataFileType\", bound=dict)\n",
    "\n",
    "class DirectoryProtocol(Protocol):\n",
    "    archive_local: str\n",
    "    output_folder: str\n",
    "\n",
    "    def set_output_dirs(self, target_name: str, fileid: int, create: bool = True) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def image_path(self, image_path: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def photometry_path(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_as(self, filename: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def log_path(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def from_fid(cls, fid: int) -> 'DirectoryProtocol':\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Directories:\n",
    "    \"\"\"\n",
    "    Class for creating input and output directories, given a configparser instance.\n",
    "    Overwrite archive_local or output_folder to manually place elsewhere.\n",
    "    \"\"\"\n",
    "    archive_local = LOCAL_ARCHIEVE \n",
    "    output_folder = FOLDER_OUTPUT\n",
    "    \n",
    "    #######\n",
    "        #OLD CODE:\n",
    "    #archive_local: Optional[str] = None\n",
    "    #output_folder: Optional[str] = None\n",
    "\n",
    "    #######\n",
    "\n",
    "    def __init__(self, config: Optional[ConfigParser] = None):\n",
    "        self.config = config or utils.load_config()\n",
    "\n",
    "    def set_output_dirs(self, target_name: str, fileid: int, create: bool = True,\n",
    "                        output_folder_root: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        The function is meant to be called from within a context where a\n",
    "        target_name and fileid are defined, so that the output_folder\n",
    "        can be created appropriately.\n",
    "\n",
    "        Parameters:\n",
    "            target_name (str): Target name.\n",
    "            fileid (int): The fileid of the file being processed\n",
    "            create (bool): Whether to create the output_folder if it doesn't exist.\n",
    "            output_folder_root (str): Overwrite the root directory for output.\n",
    "        \"\"\"\n",
    "\n",
    "        # Checking for None allows manual declarations to not be overwritten.\n",
    "        if self.archive_local is None:\n",
    "            self.archive_local = self._set_archive()\n",
    "\n",
    "        self.output_folder = self._set_output(target_name, fileid, output_folder_root)\n",
    "\n",
    "        # Create output folder if necessary.\n",
    "        if create:\n",
    "            os.makedirs(self.output_folder, exist_ok=True)\n",
    "            logger.info(\"Placing output in '%s'\", self.output_folder)\n",
    "\n",
    "    def _set_archive(self) -> Optional[str]:\n",
    "        archive_local = self.config.get('photometry', 'archive_local', fallback=None)\n",
    "        if archive_local is not None and not os.path.isdir(archive_local):\n",
    "            raise FileNotFoundError(\"ARCHIVE is not available: \" + archive_local)\n",
    "        logger.info(f\"Using data from: {archive_local}.\")\n",
    "        return archive_local\n",
    "\n",
    "    def _set_output(self, target_name: str, fileid: int, output_folder_root: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Directory for output, defaults to current\n",
    "        directory if config is invalid or empty.\n",
    "        \"\"\"\n",
    "        output_folder_root = self.config.get('photometry', 'output', fallback='.') if output_folder_root is None \\\n",
    "            else output_folder_root\n",
    "        output_folder = os.path.join(output_folder_root, target_name, f'{fileid:05d}')\n",
    "        return output_folder\n",
    "\n",
    "    def image_path(self, image_path: str) -> str:\n",
    "        return os.path.join(self.archive_local, image_path)\n",
    "\n",
    "    @property\n",
    "    def photometry_path(self) -> str:\n",
    "        return os.path.join(self.output_folder, 'photometry.ecsv')\n",
    "\n",
    "    def save_as(self, filename: str) -> str:\n",
    "        return os.path.join(self.output_folder, filename)\n",
    "\n",
    "    @property\n",
    "    def log_path(self) -> str:\n",
    "        return os.path.join(self.output_folder, \"photometry.log\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_fid(cls, fid: int, config: Optional[ConfigParser] = None, create: bool = True,\n",
    "                 datafile: Optional[Dict] = None) -> DirectoryProtocol:\n",
    "        instance = cls(config)\n",
    "        datafile = datafile or api.get_datafile(fid)\n",
    "        instance.set_output_dirs(datafile['target_name'], fid, create)\n",
    "        return instance\n",
    "\n",
    "\n",
    "class DirectoriesDuringTest:\n",
    "    \"\"\"\n",
    "    Directory class in testing config.\n",
    "    \"\"\"\n",
    "    archive_local = None\n",
    "    output_folder = None\n",
    "\n",
    "    def __init__(self, input_dir: Union[str, Path], output_dir: Union[str, Path]):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def set_output_dirs(self, target_name: str, fileid: int, create: bool = True) -> None:\n",
    "        self.output_folder = os.path.join(self.output_dir, target_name, f'{fileid:05d}')\n",
    "        if create:\n",
    "            os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "    def image_path(self, image_path: str) -> str:\n",
    "        return os.path.join(self.input_dir+image_path)\n",
    "\n",
    "    @property\n",
    "    def photometry_path(self) -> str:\n",
    "        return os.path.join(self.output_folder, 'photometry.ecsv')\n",
    "\n",
    "    def save_as(self, filename: str) -> str:\n",
    "        return os.path.join(self.output_folder, filename)\n",
    "\n",
    "    @property\n",
    "    def log_path(self) -> str:\n",
    "        return os.path.join(self.output_folder, \"photometry.log\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_fid(cls, fid: int, input_dir: Union[str, Path] = './test/input',\n",
    "                 output_dir: Union[str, Path] = './test/output', datafile: Optional[Dict] = None) -> 'DirectoryProtocol':\n",
    "        instance = cls(input_dir, output_dir)\n",
    "        datafile = datafile or api.get_datafile(fid)\n",
    "        instance.set_output_dirs(datafile['target_name'], fid)\n",
    "        return instance\n",
    "\n",
    "\n",
    "class IOManager:\n",
    "    \"\"\"\n",
    "    Implement a runner to shuffle data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target: Target,\n",
    "                 directories: DirectoryProtocol,\n",
    "                 datafile: Dict):\n",
    "        self.target = target\n",
    "        self.directories = directories\n",
    "        self.output_folder = directories.output_folder\n",
    "        self.archive_local = directories.archive_local\n",
    "        self.datafile = datafile\n",
    "        self.diff_image_exists = False\n",
    "\n",
    "    def _load_image(self, image_path: str) -> FlowsImage:\n",
    "        \"\"\"\n",
    "        Load an image from a file.\n",
    "        \"\"\"\n",
    "        # Load the image from the FITS file:\n",
    "        image = load_image(self.directories.image_path(image_path), target_coord=self.target.coords)\n",
    "        return image\n",
    "\n",
    "    def load_science_image(self, image_path: Optional[str] = None) -> FlowsImage:\n",
    "        image_path = image_path or self.datafile['path']\n",
    "        image = self._load_image(image_path)\n",
    "        logger.info(\"Load image '%s'\", self.directories.image_path(image_path))\n",
    "        image.fid = self.datafile['fileid']\n",
    "        image.template_fid = None if self.datafile.get('template') is None else self.datafile['template']['fileid']\n",
    "        return image\n",
    "\n",
    "    def get_filter(self):\n",
    "        return get_reference_filter(self.target.photfilter)\n",
    "\n",
    "    def load_references(self, catalog: Optional[Dict] = None) -> refclean.References:\n",
    "        use_filter = self.get_filter()\n",
    "        references = api.get_catalog(self.target.name)['references'] if catalog is None else catalog['references']\n",
    "        references.sort(use_filter)\n",
    "        # Check that there actually are reference stars in that filter:\n",
    "        if allnan(references[use_filter]):\n",
    "            raise ValueError(\"No reference stars found in current photfilter.\")\n",
    "        return refclean.References(table=references)\n",
    "\n",
    "    def load_diff_image(self) -> Optional[FlowsImage]:\n",
    "        diffimage_df = self.datafile.get('diffimg', None)\n",
    "        diffimage_path = diffimage_df.get('path', None) if diffimage_df else None\n",
    "        self.diff_image_exists = diffimage_path is not None\n",
    "        if diffimage_df and not self.diff_image_exists:\n",
    "            logger.warning(\"Diff image present but without path, skipping diff image photometry\")\n",
    "        if self.diff_image_exists:\n",
    "            diffimage = self._load_image(diffimage_path)\n",
    "            logger.info(\"Load difference image '%s'\", self.directories.image_path(diffimage_path))\n",
    "            diffimage.fid = diffimage_df['fileid']\n",
    "            return diffimage\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def from_fid(cls, fid: int, directories: Optional[DirectoryProtocol] = None,\n",
    "                 create_directories: bool = True, datafile: Optional[Dict] = None) -> 'IOManager':\n",
    "        \"\"\"\n",
    "        Create an IOManager from a fileid.\n",
    "        \"\"\"\n",
    "        datafile = datafile or api.get_datafile(fid)\n",
    "        target = Target.from_fid(fid=fid, datafile=datafile)\n",
    "        directories = directories or Directories.from_fid(fid=fid, create=create_directories, datafile=datafile)\n",
    "        return cls(target=target, directories=directories, datafile=datafile)\n",
    "\n",
    "\n",
    "def del_dir(target: Union[Path, str],\n",
    "            only_if_empty: bool = False,\n",
    "            delete_parent_if_file: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Delete a given directory and its subdirectories.\n",
    "    Ex: If filename is the path to a file: `del_dir(Path(filename).parent)`\n",
    "\n",
    "    :param target: The directory to delete\n",
    "    :param only_if_empty: Raise RuntimeError if any file is found in the tree\n",
    "    :param delete_parent_if_file: Delete the parent directory if it is a file\n",
    "    \"\"\"\n",
    "    target = Path(target).expanduser()\n",
    "    if not target.exists():\n",
    "        logger.warning(\"Not deleted: Directory '%s' does not exist\", target)\n",
    "        return\n",
    "    if not target.is_dir() and delete_parent_if_file:\n",
    "        logger.warning(\"Not deleted: '%s' is not a directory and delete_parent_if_file was False\", target)\n",
    "        return\n",
    "    for p in sorted(target.glob('**/*'), reverse=True):  # This should also work on Windows (fingers crossed).\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        p.chmod(0o666)  # This should also work on Windows but we should have read/write permissions anyway\n",
    "        if p.is_dir():\n",
    "            p.rmdir()\n",
    "        else:\n",
    "            if only_if_empty:\n",
    "                raise RuntimeError(f'{p.parent} is not empty!')\n",
    "            p.unlink()\n",
    "    target.rmdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149243f4",
   "metadata": {},
   "source": [
    "# background.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dae1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "from astropy.stats import SigmaClip\n",
    "from numpy.typing import ArrayLike\n",
    "from photutils import Background2D, SExtractorBackground\n",
    "from photutils.utils import calc_total_error\n",
    "\n",
    "class FlowsBackground:\n",
    "\n",
    "    def __init__(self, background_estimator: Background2D = Background2D):\n",
    "        self.background_estimator = Background2D\n",
    "        self.background: Optional[ArrayLike] = None\n",
    "        self.background_rms: Optional[ArrayLike] = None\n",
    "\n",
    "    def estimate_background(self, clean_image: np.ma.MaskedArray) -> None:\n",
    "        # Estimate image background:\n",
    "        # Not using image.clean here, since we are redefining the mask anyway\n",
    "        bkg2d = self.background_estimator(clean_image, (128, 128), filter_size=(5, 5),\n",
    "                                         sigma_clip=SigmaClip(sigma=3.0), bkg_estimator=SExtractorBackground(),\n",
    "                                         exclude_percentile=50.0)\n",
    "        self.background = bkg2d.background\n",
    "        self.background_rms = bkg2d.background_rms\n",
    "\n",
    "    def background_subtract(self, clean_image: ArrayLike) -> ArrayLike:\n",
    "        if self.background is None:\n",
    "            self.estimate_background(clean_image)\n",
    "        return clean_image - self.background\n",
    "\n",
    "    def error(self, clean_image: ArrayLike, error_method: Callable = calc_total_error):\n",
    "        \"\"\"\n",
    "        Calculate the 2D error using the background RMS.\n",
    "        \"\"\"\n",
    "        if self.background is None:\n",
    "            raise AttributeError(\"background must be estimated before calling error\")\n",
    "        return error_method(clean_image, self.background_rms, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96937298",
   "metadata": {},
   "source": [
    "# ----------!!! photometry.py code starts here !!! ----------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db19c4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything above works!\n"
     ]
    }
   ],
   "source": [
    "print(\"everything above works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7672bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flows photometry code.\n",
    "\n",
    ".. codeauthor:: Rasmus Handberg <rasmush@phys.au.dk>\n",
    ".. codeauthor:: Emir Karamehmetoglu <emir.k@phys.au.dk>\n",
    ".. codeauthor:: Simon Holmbo <sholmbo@phys.au.dk>\n",
    "\"\"\"\n",
    "\n",
    "__version__ = get_version(pep440=False)\n",
    "\n",
    "PhotutilsBackground = TypeVar('PhotutilsBackground', bound=photutils.background.core.BackgroundBase)\n",
    "logger = create_logger()\n",
    "\n",
    "\n",
    "# def get_datafile(fileid: int) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Get datafile from API, log it, return.\n",
    "#     \"\"\"\n",
    "#     datafile = api.get_datafile(fileid)\n",
    "#     logger.debug(\"Datafile: %s\", datafile)\n",
    "#     return datafile\n",
    "\n",
    "\n",
    "# def get_catalog(targetid: int) -> Dict:\n",
    "#     catalog = api.get_catalog(targetid, output='table')\n",
    "#     logger.debug(f\"catalog obtained for target: {targetid}\")\n",
    "#     return catalog\n",
    "\n",
    "\n",
    "class PSFBuilder:\n",
    "    init_cutout_size: int = 29\n",
    "    min_pixels: int = 15\n",
    "\n",
    "    def __init__(self, image: FlowsImage, target: Target, fwhm_guess: float,\n",
    "                 epsf_builder: FlowsEPSFBuilder = FlowsEPSFBuilder):\n",
    "        self.image = image\n",
    "        self.target = target\n",
    "        self.fwhm = fwhm_guess\n",
    "        self.epsf_builder = epsf_builder\n",
    "\n",
    "        # To be updated later.\n",
    "        self.epsf = None\n",
    "\n",
    "    # @TODO: Move to PhotometryMediator\n",
    "    @property\n",
    "    def star_size(self) -> int:\n",
    "        # Make cutouts of stars using extract_stars:\n",
    "        # Scales with FWHM\n",
    "        size = int(np.round(self.init_cutout_size * self.fwhm / 6))\n",
    "        size += 0 if size % 2 else 1  # Make sure it's odd\n",
    "        size = max(size, self.min_pixels)  # Never go below 15 pixels\n",
    "        return size\n",
    "\n",
    "    def extract_star_cutouts(self, star_xys: np.ndarray) -> List[np.ma.MaskedArray]:\n",
    "        \"\"\"\n",
    "        Extract star cutouts from the image.\n",
    "        \"\"\"\n",
    "        # Extract stars from image\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', AstropyUserWarning)\n",
    "            stars = extract_stars(NDData(data=self.image.subclean.data, mask=self.image.mask),\n",
    "                                  Table(star_xys, names=('x', 'y')),\n",
    "                                  size=self.star_size + 6  # +6 for edge buffer\n",
    "                                  )\n",
    "        logger.info(\"Number of stars input to ePSF builder: %d\", len(stars))\n",
    "        return stars\n",
    "\n",
    "    def make_epsf(self, stars):\n",
    "        \"\"\"\n",
    "        Make an ePSF from the star cutouts.\n",
    "        \"\"\"\n",
    "        # Build ePSF\n",
    "        logger.info(\"Building ePSF...\")\n",
    "        builder = self.epsf_builder(\n",
    "            oversampling=1, shape=1 * self.star_size,\n",
    "            fitter=EPSFFitter(fit_boxsize=max(int(np.round(1.5 * self.fwhm)), 5)),\n",
    "            recentering_boxsize=max(int(np.round(2 * self.fwhm)), 5),\n",
    "            norm_radius=max(self.fwhm, 5), maxiters=100,\n",
    "            progress_bar=multiprocessing.parent_process() is None and logger.getEffectiveLevel() <= 20\n",
    "        )\n",
    "        epsf, stars = builder(stars)\n",
    "\n",
    "        logger.info(f\"Built PSF model \"\n",
    "                    f\"{epsf.fit_info['n_iter'] / epsf.fit_info['max_iters']} in {epsf.fit_info['time']} seconds\")\n",
    "\n",
    "        return epsf, stars\n",
    "\n",
    "\n",
    "class Photometry:\n",
    "\n",
    "    def __init__(self, photometry_obj: Optional[BasicPSFPhotometry] = None):\n",
    "        self.photometry_obj = photometry_obj\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_appflux(apphot_tbl: Table, apertures: CircularAperture, annuli: CircularAnnulus) -> Table:\n",
    "        \"\"\"\n",
    "        Calculate the aperture flux for the given apertures and annuli and append result to table.\n",
    "        \"\"\"\n",
    "        # Subtract background estimated from annuli:\n",
    "        bkg = (apphot_tbl['aperture_sum_1'] / annuli.area) * apertures.area\n",
    "        apphot_tbl['flux_aperture'] = apphot_tbl['aperture_sum_0'] - bkg\n",
    "\n",
    "        apphot_tbl['flux_aperture_error'] = np.sqrt(apphot_tbl['aperture_sum_err_0'] ** 2 +\n",
    "                                                    (apphot_tbl[\n",
    "                                                         'aperture_sum_err_1'] / annuli.area * apertures.area) ** 2)\n",
    "        return apphot_tbl\n",
    "\n",
    "    def apphot(self, coordinates: ArrayLike, image: FlowsImage, fwhm: float, use_raw: bool = False) -> Table:\n",
    "        img = image.clean if use_raw else image.subclean\n",
    "        apertures = CircularAperture(coordinates, r=fwhm)\n",
    "        annuli = CircularAnnulus(coordinates, r_in=1.5 * fwhm, r_out=2.5 * fwhm)\n",
    "        apphot_tbl = aperture_photometry(img, [apertures, annuli], mask=image.mask, error=image.error)\n",
    "        return self.calculate_appflux(apphot_tbl, apertures, annuli)\n",
    "\n",
    "    def create_photometry_object(self, fwhm: Union[float, u.Quantity], psf_model: photutils.psf.EPSFModel,\n",
    "                                 fitsize: Union[int, Tuple[int]], fitter: Callable = fitting.LevMarLSQFitter(),\n",
    "                                 bkg: PhotutilsBackground = MedianBackground()):\n",
    "        self.photometry_obj = BasicPSFPhotometry(group_maker=DAOGroup(fwhm), bkg_estimator=bkg, psf_model=psf_model,\n",
    "                                                 fitter=fitter, fitshape=fitsize, aperture_radius=fwhm)\n",
    "\n",
    "    def psfphot(self, image: ArrayLike, init_table: Table) -> Tuple[BasicPSFPhotometry, Table]:\n",
    "        \"\"\"PSF photometry on init guesses table/row.\n",
    "        \"\"\"\n",
    "        if self.photometry_obj is None:\n",
    "            raise ValueError('Photometry object not initialized.')\n",
    "        # logger.info(f\"{init_table}\")\n",
    "        output: Table = self.photometry_obj(image=image, init_guesses=init_table)\n",
    "        return self.photometry_obj, output\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_flux_error(phot_tables: Dict[int, Table],\n",
    "                           flux: float, flux_err: float, exptime: float) -> tuple[float, float]:\n",
    "        \"\"\"Rescale the error using input phot_tables dict with keys as input fit shapes and values as tables/rows\n",
    "        returning the new error estimate and its fit shape.\"\"\"\n",
    "        select_first_row = isinstance(list(phot_tables.values())[0], Table)\n",
    "\n",
    "        for fit_shape, row in phot_tables.items():\n",
    "            row = row[0] if select_first_row else row\n",
    "            new_err = row['flux_unc'] / exptime\n",
    "            new_flux = row['flux_fit'] / exptime\n",
    "            if new_flux <= flux + flux_err:\n",
    "                return fit_shape, new_err\n",
    "        logger.warning(\"Rescaled psf flux errors do not overlap input flux + error to 1 sigma, using original error.\")\n",
    "        return 0, flux_err\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fit_shapes(fwhm: Union[float, int], star_size: int, fwhm_min: int = 2,\n",
    "                       fwhm_max: int = 4) -> NDArray[np.int_]:\n",
    "        if star_size / fwhm < fwhm_min:\n",
    "            return np.atleast_1d(np.array(star_size))\n",
    "        fit_shapes = np.arange(int(fwhm_min * fwhm) - 1, min(int(fwhm_max * fwhm), star_size), 1)\n",
    "        return fit_shapes[fit_shapes % 2 == 1]  # odd\n",
    "\n",
    "\n",
    "class PhotometryManager:\n",
    "\n",
    "    def __init__(self, target: Target, image: FlowsImage, bkg: FlowsBackground,\n",
    "                 references: References, directories: DirectoryProtocol,\n",
    "                 fwhm: Optional[float] = None,\n",
    "                 psf_builder: Optional[PSFBuilder] = None,\n",
    "                 cleaner: Optional[ReferenceCleaner] = None,\n",
    "                 diffimage: Optional[FlowsImage] = None):\n",
    "        self.target = target\n",
    "        self.image = image\n",
    "        self.bkg = bkg\n",
    "        self.references = references\n",
    "        self.directories = directories\n",
    "        # Initially possibly None:\n",
    "        self.fwhm = fwhm\n",
    "        self.psf_builder = psf_builder\n",
    "        self.cleaner = cleaner\n",
    "        self.diffimage = diffimage\n",
    "        # To be updated later.\n",
    "        self.clean_references = references\n",
    "        self.clean_references_with_diff = references\n",
    "        self.init_guesses = None\n",
    "        self.init_guesses_diff = None\n",
    "        self.photometry = Photometry()\n",
    "        self.diff_im_exists = diffimage is not None\n",
    "        self.results_table = None\n",
    "\n",
    "    def propogate_references(self):\n",
    "        self.references.make_sky_coords()  # Make sky coordinates\n",
    "        self.references.propagate(self.image.obstime)  # get reference catalog at obstime\n",
    "\n",
    "    def background_subtract(self):\n",
    "        self.image.subclean = self.bkg.background_subtract(self.image.clean)\n",
    "        self.image.error = self.bkg.error(self.image.clean)\n",
    "        if self.diff_im_exists:\n",
    "            self.diffimage.error = self.image.error\n",
    "\n",
    "    def recalculate_image_wcs(self, cm_timeout: float):\n",
    "        self.image = correct_wcs(self.image, self.references, target=self.target, timeout=cm_timeout)\n",
    "\n",
    "    def calculate_pixel_coordinates(self):\n",
    "        # Calculate pixel-coordinates of references:\n",
    "        self.references.get_xy(self.image.wcs)\n",
    "        self.references.make_pixel_columns()\n",
    "\n",
    "    def clean_reference_stars(self, rsq_min: float = 0.15):\n",
    "        # Clean out the references:\n",
    "        self.cleaner = ReferenceCleaner(self.image, self.references, rsq_min=rsq_min)\n",
    "        # Reject references that are too close to target or edge of the image\n",
    "        masked_references = self.cleaner.mask_edge_and_target(self.target.coords)\n",
    "        if not masked_references.table:\n",
    "            raise RuntimeError(\"No clean references in field\")\n",
    "\n",
    "        # Clean masked reference star locations\n",
    "        self.clean_references, self.fwhm = self.cleaner.clean_references(masked_references)\n",
    "\n",
    "    def create_psf_builder(self) -> PSFBuilder:\n",
    "        return PSFBuilder(self.image, self.target, self.fwhm)\n",
    "\n",
    "    def update_reference_with_epsf(self, stars):\n",
    "        # Store which stars were used in ePSF in the table:\n",
    "        self.clean_references.table.add_column(col=[False], name='used_for_epsf')\n",
    "        self.clean_references.table['used_for_epsf'][[star.id_label - 1 for star in stars.all_good_stars]] = True\n",
    "        logger.info(\"Number of stars used for ePSF: %d\", np.sum(self.clean_references.table['used_for_epsf']))\n",
    "\n",
    "    def create_epsf(self, psf_builder: PSFBuilder = None):\n",
    "        if psf_builder is None:\n",
    "            psf_builder = self.create_psf_builder()\n",
    "\n",
    "        # EPSF creation\n",
    "        star_cutouts = psf_builder.extract_star_cutouts(self.cleaner.gaussian_xys)\n",
    "        epsf, stars = psf_builder.make_epsf(star_cutouts)\n",
    "        epsf_ok, epsf_fwhms = verify_epsf(epsf)\n",
    "        if not epsf_ok:\n",
    "            raise RuntimeError(\"Bad ePSF detected.\")\n",
    "        psf_builder.epsf = epsf\n",
    "\n",
    "        # Use the largest FWHM as new FWHM\n",
    "        fwhm = np.max(epsf_fwhms)\n",
    "        logger.info(f\"Final FWHM based on ePSF: {fwhm}\")\n",
    "        psf_builder.fwhm = fwhm\n",
    "\n",
    "        # Update state\n",
    "        self.fwhm = fwhm\n",
    "        self.psf_builder = psf_builder\n",
    "        self.update_reference_with_epsf(stars)\n",
    "\n",
    "    def prepare_target_and_references_for_photometry(self):\n",
    "        # position in the image including target as row 0:\n",
    "        self.target.calc_pixels(self.image.wcs)\n",
    "        self.clean_references.add_target(self.target, starid=0)  # by default prepends target\n",
    "        self.init_guesses = InitGuess(self.clean_references, target_row=0)\n",
    "        if self.diff_im_exists:\n",
    "            self.clean_references_with_diff = copy(self.clean_references)\n",
    "            self.clean_references_with_diff.add_target(self.target, starid=-1)\n",
    "        self.init_guesses_diff = InitGuess(self.clean_references_with_diff, target_row=1, diff_row=0)\n",
    "\n",
    "    def apphot(self) -> Table:\n",
    "        # apphot_tbl.insert_row(0, dict(diff_apphot_tbl[0]))\n",
    "        apphot_tbl = self.photometry.apphot(self.clean_references.xy, self.image, self.fwhm)\n",
    "        if self.diff_im_exists:\n",
    "            # Add diff image photometry:\n",
    "            diff_tbl = self.photometry.apphot(self.clean_references.xy[0], self.diffimage, self.fwhm, use_raw=True)\n",
    "            apphot_tbl.insert_row(0, dict(diff_tbl[0]))\n",
    "        return apphot_tbl\n",
    "\n",
    "    def psfphot(self, fit_shape: Optional[Union[int, Tuple[int, int]]] = None) -> Table:\n",
    "        fit_shape = self.psf_builder.star_size if fit_shape is None else fit_shape\n",
    "        # PSF photometry:\n",
    "        self.photometry.create_photometry_object(fwhm=self.fwhm, psf_model=self.psf_builder.epsf, fitsize=fit_shape)\n",
    "        psfphot_tbl = self.raw_psf_phot()\n",
    "        if self.diff_im_exists:\n",
    "            # Add diff image photometry:\n",
    "            diff_tbl = self.diff_psf_phot()\n",
    "            psfphot_tbl.insert_row(0, dict(diff_tbl[0]))\n",
    "        return psfphot_tbl\n",
    "\n",
    "    def raw_psf_phot(self, init_guess: Optional[Table] = None) -> Table:\n",
    "        init_guess = self.init_guesses.init_guess_full if init_guess is None else init_guess\n",
    "        _, psf_tbl = self.photometry.psfphot(image=self.image.subclean, init_table=init_guess)\n",
    "        return psf_tbl\n",
    "\n",
    "    def diff_psf_phot(self) -> Table:\n",
    "        _, psf_tbl = self.photometry.psfphot(image=self.diffimage.clean,\n",
    "                                             init_table=self.init_guesses_diff.init_guess_diff)\n",
    "        return psf_tbl\n",
    "\n",
    "    def rescale_uncertainty(self, psfphot_tbl: Table, dynamic: bool = True, \n",
    "                            static_fwhm: float = 2.5, epsilon_mag: float = 0.004,\n",
    "                            ensure_greater: bool = True):\n",
    "        \"\"\"\n",
    "        Rescale the uncertainty of the PSF photometry using a variable fitsize.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psfphot_tbl : Table\n",
    "            Table of PSF photometry.\n",
    "        dynamic : bool\n",
    "            Dynamically decide FWHM multiple for rescaling.\n",
    "        static_fwhm : float\n",
    "            FWHM multiple to use incase dynamic fails or don't want to use it. Default 2.5 determined empirically.\n",
    "        epsilon_mag : float\n",
    "            Small magnitude change within which new and old uncertainties are considered the same. \n",
    "            Should be smaller than ~1/2 the expected uncertainty.\n",
    "        \"\"\"\n",
    "        # Rescale psf errors from fit iteratively\n",
    "        fit_shapes = self.photometry.get_fit_shapes(self.fwhm, self.psf_builder.star_size)\n",
    "        fit_shape = int(static_fwhm * self.fwhm)\n",
    "        fit_shape = fit_shape if fit_shape % 2 == 1 else fit_shape + 1\n",
    "        if dynamic and len(fit_shapes) > 1:\n",
    "            _phot_tables_dict = {}\n",
    "            for fitshape in fit_shapes:\n",
    "                self.photometry.create_photometry_object(\n",
    "                    fwhm=self.fwhm, psf_model=self.psf_builder.epsf, fitsize=fitshape)\n",
    "                if self.diff_im_exists:\n",
    "                    _table = self.diff_psf_phot()\n",
    "                _table = self.raw_psf_phot(self.init_guesses.init_guess_target)\n",
    "                if \"flux_unc\" in _table.colnames:\n",
    "                    _phot_tables_dict[fitshape] = _table\n",
    "\n",
    "            if len(_phot_tables_dict) == 0:\n",
    "                logger.warning(\"No PSF errors found for dynamic rescaling, trying static.\")\n",
    "                dynamic = False\n",
    "            else:\n",
    "                # Find the fit shape elbow:\n",
    "                flux = psfphot_tbl[0]['flux_fit']\n",
    "                flux_err = psfphot_tbl[0]['flux_unc']\n",
    "                exptime = self.image.exptime\n",
    "                dynamic_fit_shape, new_err = self.photometry.rescale_flux_error(_phot_tables_dict, flux, flux_err,\n",
    "                                                                                exptime)\n",
    "                fit_shape = dynamic_fit_shape if dynamic_fit_shape != 0 else fit_shape\n",
    "\n",
    "        # Recalculate all reference uncertainties using new fitsize:\n",
    "        logger.info(f\"Recalculating all reference uncertainties using new fitsize:\"\n",
    "                    f\" {fit_shape} pixels, ({fit_shape/self.fwhm if dynamic else static_fwhm :.2} * FWHM).\")\n",
    "        psfphot_tbl_rescaled = self.psfphot(fit_shape)\n",
    "        if psfphot_tbl['flux_unc'][0] > psfphot_tbl_rescaled['flux_unc'][0] + epsilon_mag and ensure_greater:\n",
    "            logger.info(\"Recalculated uncertainties were smaller than original and ``ensure_greater`` was True:\"\n",
    "                        \"Not using rescaled uncertainties for the SN.\")\n",
    "            psfphot_tbl['flux_unc'][1:] = psfphot_tbl_rescaled['flux_unc'][1:]\n",
    "            return psfphot_tbl\n",
    "\n",
    "        psfphot_tbl['flux_unc'] = psfphot_tbl_rescaled['flux_unc']\n",
    "        return psfphot_tbl\n",
    "\n",
    "    def make_result_table(self, psfphot_tbl: Table, apphot_tbl: Table):\n",
    "        # Build results table:\n",
    "        clean_references = self.clean_references_with_diff if self.diff_im_exists else self.clean_references\n",
    "        self.results_table = ResultsTable.make_results_table(clean_references.table, apphot_tbl, psfphot_tbl,\n",
    "                                                             self.image)\n",
    "\n",
    "    def calculate_mag(self, make_plot: bool = False) -> Tuple[Optional[plt.Figure], Optional[plt.Axes]]:\n",
    "        if self.results_table is None:\n",
    "            raise ValueError(\"Results table is not initialized. Run photometry first.\")\n",
    "        # Todo: refactor.\n",
    "        # Get instrumental magnitude (currently we do too much here).\n",
    "        results_table, mag_fig, mag_ax = instrumental_mag(self.results_table, self.target, make_plot)\n",
    "        self.results_table = results_table\n",
    "        return mag_fig, mag_ax\n",
    "\n",
    "    def calculate_pixel_scale(self):\n",
    "        # Find the pixel-scale of the science image\n",
    "        pixel_area = proj_plane_pixel_area(self.image.wcs.celestial)\n",
    "        pixel_scale = np.sqrt(pixel_area) * 3600  # arcsec/pixel\n",
    "        logger.info(\"Science image pixel scale: %f\", pixel_scale)\n",
    "        return pixel_scale\n",
    "\n",
    "    def add_metadata(self):\n",
    "        # Add metadata to the results table:\n",
    "        self.results_table.meta['fileid'] = self.image.fid\n",
    "        self.results_table.meta['target_name'] = self.target.name\n",
    "        self.results_table.meta['version'] = __version__\n",
    "        self.results_table.meta['template'] = self.image.template_fid\n",
    "        self.results_table.meta['diffimg'] = self.diffimage.fid if self.diff_im_exists else None\n",
    "        self.results_table.meta['photfilter'] = self.target.photfilter\n",
    "        self.results_table.meta['fwhm'] = self.fwhm * u.pixel\n",
    "        pixel_scale = self.calculate_pixel_scale()\n",
    "        self.results_table.meta['pixel_scale'] = pixel_scale * u.arcsec / u.pixel\n",
    "        self.results_table.meta['seeing'] = (self.fwhm * pixel_scale) * u.arcsec\n",
    "        self.results_table.meta['obstime-bmjd'] = float(self.image.obstime.mjd)\n",
    "        self.results_table.meta['used_wcs'] = str(self.image.wcs)\n",
    "\n",
    "    @classmethod\n",
    "    def create_from_fid(cls, fid: int, directories: Optional[DirectoryProtocol] = None,\n",
    "                        create_directories: bool = True, datafile: Optional[Dict] = None) -> 'PhotometryManager':\n",
    "        \"\"\"\n",
    "        Create a Photometry object from a fileid.\n",
    "        \"\"\"\n",
    "        io = IOManager.from_fid(fid, directories=directories, create_directories=create_directories, datafile=datafile)\n",
    "        return PhotometryManager(target=io.target, image=io.load_science_image(), bkg=FlowsBackground(),\n",
    "                                 references=io.load_references(), directories=io.directories,\n",
    "                                 diffimage=io.load_diff_image())\n",
    "\n",
    "\n",
    "def do_phot(fileid: int, cm_timeout: Optional[float] = None, make_plots: bool = True,\n",
    "            directories: Optional[DirectoryProtocol] = None, datafile: Optional[Dict[str, Any]] = None,\n",
    "            rescale: bool = True, rescale_dynamic: bool = True) -> ResultsTable:\n",
    "    # Set up photometry runner\n",
    "    pm = PhotometryManager.create_from_fid(fileid, directories=directories, datafile=datafile, create_directories=True)\n",
    "\n",
    "    # Set up photometry\n",
    "    pm.propogate_references()\n",
    "    pm.background_subtract()\n",
    "    pm.recalculate_image_wcs(cm_timeout if cm_timeout is not None else np.inf)\n",
    "    pm.calculate_pixel_coordinates()\n",
    "    pm.clean_reference_stars()\n",
    "    pm.create_epsf()  # Using default EPSF constructor, could pass in custom.\n",
    "    pm.prepare_target_and_references_for_photometry()  # Add target to reference table\n",
    "\n",
    "    # Do photometry\n",
    "    apphot_tbl = pm.apphot()\n",
    "    # Verify uncertainty exists after PSF phot:\n",
    "    psfphot_tbl = ResultsTable.verify_uncertainty_column(pm.psfphot())  \n",
    "    if rescale:  # Rescale uncertainties\n",
    "        psfphot_tbl = pm.rescale_uncertainty(psfphot_tbl, dynamic=rescale_dynamic) \n",
    "\n",
    "    # Build results table and calculate magnitudes\n",
    "    pm.make_result_table(psfphot_tbl, apphot_tbl)\n",
    "    mag_fig, mag_ax = pm.calculate_mag(make_plot=make_plots)\n",
    "    pm.add_metadata()\n",
    "\n",
    "    if make_plots:\n",
    "        do_plots(pm, mag_fig, mag_ax)\n",
    "    return pm.results_table\n",
    "\n",
    "\n",
    "def timed_photometry(fileid: int, cm_timeout: Optional[float] = None, make_plots: bool = True,\n",
    "                     directories: Optional[DirectoryProtocol] = None, save: bool = True,\n",
    "                     datafile: Optional[Dict[str, Any]] = None, rescale: bool = True,\n",
    "                     rescale_dynamic: bool = True) -> ResultsTable:\n",
    "    # TODO: Timer should be moved out of this function.\n",
    "    tic = default_timer()\n",
    "    results_table = do_phot(fileid, cm_timeout, make_plots, directories, datafile, rescale, rescale_dynamic)\n",
    "\n",
    "    # Save the results table:\n",
    "    if save:\n",
    "        results_table.write(directories.photometry_path, format='ascii.ecsv', delimiter=',', overwrite=True)\n",
    "\n",
    "    # Log result and time taken:\n",
    "    logger.info(\"------------------------------------------------------\")\n",
    "    logger.info(\"Success!\")\n",
    "    logger.info(\"Main target: %f +/- %f\", results_table[0]['mag'], results_table[0]['mag_error'])\n",
    "    logger.info(\"Photometry took: %.1f seconds\", default_timer() - tic)\n",
    "    return results_table\n",
    "def do_plots(pm: PhotometryManager, mag_fig: plt.Figure, mag_ax: plt.Axes):\n",
    "    # Plot the image:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 18))\n",
    "    plot_image(pm.image.clean, ax=ax[0], scale='log', cbar='right', title='Image')\n",
    "    plot_image(pm.image.mask, ax=ax[1], scale='linear', cbar='right', title='Mask')\n",
    "    fig.savefig(pm.directories.save_as('original.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Plot background estimation:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    plot_image(pm.image.clean, ax=ax[0], scale='log', title='Original')\n",
    "    plot_image(pm.bkg.background, ax=ax[1], scale='log', title='Background')\n",
    "    plot_image(pm.image.subclean, ax=ax[2], scale='log', title='Background subtracted')\n",
    "    fig.savefig(pm.directories.save_as('background.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Create plot of target and reference star positions:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 18))\n",
    "    plot_image(pm.image.subclean, ax=ax, scale='log', cbar='right', title=pm.target.name)\n",
    "    ax.scatter(pm.references.table['pixel_column'], pm.references.table['pixel_row'], c='r', marker='o', alpha=0.6)\n",
    "    ax.scatter(pm.cleaner.gaussian_xys[:, 0], pm.cleaner.gaussian_xys[:, 1], marker='s', alpha=0.6, edgecolors='green',\n",
    "               facecolors='none')\n",
    "    ax.scatter(pm.target.pixel_column, pm.target.pixel_row, marker='+', s=20, c='r')\n",
    "    fig.savefig(pm.directories.save_as('positions.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Plot EPSF\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    plot_image(pm.psf_builder.epsf.data, ax=ax1, cmap='viridis')\n",
    "    for a, ax in ((0, ax3), (1, ax2)):\n",
    "        profile = pm.psf_builder.epsf.data.sum(axis=a)\n",
    "        ax.plot(profile, 'k.-')\n",
    "        ax.axvline(profile.argmax())\n",
    "        ax.set_xlim(-0.5, len(profile) - 0.5)\n",
    "    ax4.axis('off')\n",
    "    fig.savefig(pm.directories.save_as('epsf.png'), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    del ax, ax1, ax2, ax3, ax4\n",
    "\n",
    "    # Create two plots of the difference image:\n",
    "    if pm.diff_im_exists:\n",
    "        fig, ax = plt.subplots(1, 1, squeeze=True, figsize=(20, 20))\n",
    "        plot_image(pm.diffimage.clean, ax=ax, cbar='right', title=pm.target.name)\n",
    "        ax.plot(pm.target.pixel_column, pm.target.pixel_row, marker='+', markersize=20, color='r')\n",
    "        fig.savefig(pm.directories.save_as('diffimg.png'), bbox_inches='tight')\n",
    "        ax.set_xlim(pm.target.pixel_column - 50, pm.target.pixel_column + 50)\n",
    "        ax.set_ylim(pm.target.pixel_row - 50, pm.target.pixel_row + 50)\n",
    "        fig.savefig(pm.directories.save_as('diffimg_zoom.png'), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Calibration (handled in magnitudes.py).\n",
    "    mag_fig.savefig(pm.directories.save_as('calibration.png'), bbox_inches='tight')\n",
    "    plt.close(mag_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25606782",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_menager = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e96fdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87af00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .fileio import DirectoryProtocol, IOManager  # noqa: E402\n",
    "#from .background import FlowsBackground  # noqa: E402\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38866473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"/flows\") # go to parent dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14cb27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utilities import create_logger\n",
    "#from filters import get_reference_filter\n",
    "#import magnitudes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb07a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from magnitudes import instrumental_mag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31340b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
